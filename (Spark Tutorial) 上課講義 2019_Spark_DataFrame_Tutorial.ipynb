{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/SparkTutorial/blob/master/(Spark%20Tutorial)%20%E4%B8%8A%E8%AA%B2%E8%AC%9B%E7%BE%A9%202019_Spark_DataFrame_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFD3Nh5xZ2A-",
        "outputId": "bc9ce43a-2001-4317-e126-dbc21a46c633"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-25 04:05:12--  https://www.dropbox.com/s/cce2nnozquh8twz/init_env%20spark%203.0.1.sh?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/cce2nnozquh8twz/init_env%20spark%203.0.1.sh [following]\n",
            "--2021-11-25 04:05:12--  https://www.dropbox.com/s/raw/cce2nnozquh8twz/init_env%20spark%203.0.1.sh\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com/cd/0/inline/BakoF12MvG-aBTDpJaTfJkPAn_u4KIg7jycD5Cgvni9PdH9K-_AJ7dqmh4mIRX6fTW-OTwuUmSdjpyaoFurOsFWC2s3MMNaqPmkP8B7zHr5BdsNu7-EeayQPFPo1TQnzOrTQW93eZxEnI-GbzuiT51iI/file# [following]\n",
            "--2021-11-25 04:05:13--  https://uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com/cd/0/inline/BakoF12MvG-aBTDpJaTfJkPAn_u4KIg7jycD5Cgvni9PdH9K-_AJ7dqmh4mIRX6fTW-OTwuUmSdjpyaoFurOsFWC2s3MMNaqPmkP8B7zHr5BdsNu7-EeayQPFPo1TQnzOrTQW93eZxEnI-GbzuiT51iI/file\n",
            "Resolving uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com (uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6022:15::a27d:420f\n",
            "Connecting to uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com (uc27078e7ac6ce3df96a4743a089.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 341 [text/plain]\n",
            "Saving to: ‘init_env.sh’\n",
            "\n",
            "init_env.sh         100%[===================>]     341  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-25 04:05:13 (40.0 MB/s) - ‘init_env.sh’ saved [341/341]\n",
            "\n",
            "環境初始化完畢\n"
          ]
        }
      ],
      "source": [
        "! wget -O init_env.sh https://www.dropbox.com/s/cce2nnozquh8twz/init_env%20spark%203.1.1.sh?dl=0 && \\\n",
        "bash init_env.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "u314rKd1Z2BC"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.9-src.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "dU1UwvjtaaSk",
        "outputId": "cdb3f55e-6794-470d-f1cc-c34f64f49af8"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-f3f4f7cb3c88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    341\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 343\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    344\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-6a58bf71167f>:3 "
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEFwbXSgauLA",
        "outputId": "9d45b985-ed00-4f05-a6b9-61f1da0db2d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-11-25 04:05:35--  https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv [following]\n",
            "--2021-11-25 04:05:35--  https://www.dropbox.com/s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucf1de17698f7673545434833114.dl.dropboxusercontent.com/cd/0/inline/Bak4KV5e_FfchNV1sqXLSEBm34olnzi4yn4gTNDagHFOnV7nplHcd2l7OY-HKL0ldVVoFrZ2KpJd74WM55G_UjkeufsCZhfop63jFIVYysRCBiNq3gUtyRogBOOrUSXqa8vtiH7DPryiwSbNrkP_NWKn/file# [following]\n",
            "--2021-11-25 04:05:35--  https://ucf1de17698f7673545434833114.dl.dropboxusercontent.com/cd/0/inline/Bak4KV5e_FfchNV1sqXLSEBm34olnzi4yn4gTNDagHFOnV7nplHcd2l7OY-HKL0ldVVoFrZ2KpJd74WM55G_UjkeufsCZhfop63jFIVYysRCBiNq3gUtyRogBOOrUSXqa8vtiH7DPryiwSbNrkP_NWKn/file\n",
            "Resolving ucf1de17698f7673545434833114.dl.dropboxusercontent.com (ucf1de17698f7673545434833114.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6017:15::a27d:20f\n",
            "Connecting to ucf1de17698f7673545434833114.dl.dropboxusercontent.com (ucf1de17698f7673545434833114.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50453822 (48M) [text/plain]\n",
            "Saving to: ‘pm25.csv’\n",
            "\n",
            "pm25.csv            100%[===================>]  48.12M  70.1MB/s    in 0.7s    \n",
            "\n",
            "2021-11-25 04:05:37 (70.1 MB/s) - ‘pm25.csv’ saved [50453822/50453822]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjbkqXx0ayTw"
      },
      "outputs": [],
      "source": [
        "weather = sc.textFile(\"./pm25.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9jzWplxZ2BE"
      },
      "outputs": [],
      "source": [
        "weather_data_rdd = weather.map(lambda line : line.split(\",\"))\n",
        "pm25schema = weather_data_rdd.first()\n",
        "# print(pm25schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FisGymJZ2BM"
      },
      "source": [
        "# 回想如何使用RDD計算求取2015年，大里每小時的平均pm25數值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPMxMS1f11qI"
      },
      "outputs": [],
      "source": [
        "clean_weather_data = weather_data_rdd\\\n",
        "                    .filter(lambda x: x!=pm25schema)\\\n",
        "                    .filter(remove_row_with_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KGoSn_c3viJ"
      },
      "outputs": [],
      "source": [
        "dalipm25 = clean_weather_data.filter(lambda x: x[1] == '大里' and x[2]== \"PM2.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xzMR32zDZ2BQ",
        "outputId": "28ef79c5-7e85-447b-86ba-f0e1cf94ddb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(35.43939393939394, 11), (35.38383838383838, 12), (34.92424242424242, 10), (34.59090909090909, 13), (33.43434343434343, 14), (32.80808080808081, 9), (32.63636363636363, 21), (32.505050505050505, 20), (32.474747474747474, 15), (32.18181818181818, 22), (31.747474747474747, 19), (31.166666666666668, 16), (30.939393939393938, 23), (30.818181818181817, 17), (30.63131313131313, 18), (30.04040404040404, 0), (29.636363636363637, 8), (29.1010101010101, 1), (28.1010101010101, 2), (27.065656565656564, 3), (26.51010101010101, 7), (25.853535353535353, 4), (25.055555555555557, 5), (24.883838383838384, 6)]\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "def remove_row_with_noise (x):\n",
        "    for i in range(3, len(x)):\n",
        "        if not x[i].isdecimal():\n",
        "            return False\n",
        "    return True \n",
        "\n",
        "def hourKeyGen(x):\n",
        "    hourkeypair = []\n",
        "    x=x[3:]\n",
        "    for i, value in enumerate(x):\n",
        "      print(i, value)\n",
        "      hourkeypair.append((i, float(value)))\n",
        "    return hourkeypair\n",
        "\n",
        "count = dalipm25.count()\n",
        "HourSum = dalipm25\\\n",
        "            .flatMap(hourKeyGen)\\\n",
        "            .reduceByKey(lambda x,y: x+y)\\\n",
        "            .mapValues(lambda x: x/count)\\\n",
        "            .map(lambda x: (x[1],x[0])).top(24)\n",
        "\n",
        "print(HourSum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOldTm0PZ2BR"
      },
      "source": [
        "# 使用DataFrame 來計算每小時平均值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Peojix3wauLF",
        "outputId": "a9bb2a03-4ead-46fd-b69c-4519b9203e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['2015/01/01', '大里', 'PM2.5', '53', '55', '58', '53', '43', '36', '35', '42', '55', '64', '65', '59', '52', '44', '47', '41', '43', '40', '42', '35', '28', '20', '18', '16']\n"
          ]
        }
      ],
      "source": [
        "print(dalipm25.first())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLNcEsygc6oX"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3TQFM4tixp6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "3pl9EZixkNiT",
        "outputId": "69c051a4-7560-45ef-d1f1-60cb15ec0856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(date='2015/01/01', hr_01=53.0, hr_02=55.0, hr_03=58.0, hr_04=53.0, hr_05=43.0, hr_06=36.0, hr_07=35.0, hr_08=42.0, hr_09=55.0, hr_10=64.0, hr_11=65.0, hr_12=59.0, hr_13=52.0, hr_14=44.0, hr_15=47.0, hr_16=41.0, hr_17=43.0, hr_18=40.0, hr_19=42.0, hr_20=35.0, hr_21=28.0, hr_22=20.0, hr_23=18.0, hr_24=16.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/02', hr_01=21.0, hr_02=22.0, hr_03=26.0, hr_04=23.0, hr_05=20.0, hr_06=18.0, hr_07=15.0, hr_08=21.0, hr_09=21.0, hr_10=25.0, hr_11=29.0, hr_12=32.0, hr_13=34.0, hr_14=29.0, hr_15=32.0, hr_16=39.0, hr_17=51.0, hr_18=51.0, hr_19=47.0, hr_20=43.0, hr_21=43.0, hr_22=48.0, hr_23=47.0, hr_24=53.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/03', hr_01=48.0, hr_02=48.0, hr_03=43.0, hr_04=38.0, hr_05=37.0, hr_06=36.0, hr_07=37.0, hr_08=34.0, hr_09=37.0, hr_10=46.0, hr_11=64.0, hr_12=77.0, hr_13=83.0, hr_14=75.0, hr_15=68.0, hr_16=69.0, hr_17=64.0, hr_18=65.0, hr_19=59.0, hr_20=66.0, hr_21=71.0, hr_22=66.0, hr_23=57.0, hr_24=48.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/04', hr_01=60.0, hr_02=56.0, hr_03=53.0, hr_04=43.0, hr_05=53.0, hr_06=53.0, hr_07=52.0, hr_08=44.0, hr_09=44.0, hr_10=50.0, hr_11=49.0, hr_12=51.0, hr_13=45.0, hr_14=42.0, hr_15=40.0, hr_16=38.0, hr_17=36.0, hr_18=43.0, hr_19=51.0, hr_20=63.0, hr_21=68.0, hr_22=72.0, hr_23=66.0, hr_24=58.0, location='大里', measure='PM2.5'),\n",
              " Row(date='2015/01/05', hr_01=48.0, hr_02=42.0, hr_03=42.0, hr_04=34.0, hr_05=34.0, hr_06=28.0, hr_07=34.0, hr_08=35.0, hr_09=45.0, hr_10=47.0, hr_11=54.0, hr_12=46.0, hr_13=35.0, hr_14=19.0, hr_15=16.0, hr_16=21.0, hr_17=24.0, hr_18=28.0, hr_19=37.0, hr_20=52.0, hr_21=60.0, hr_22=62.0, hr_23=64.0, hr_24=61.0, location='大里', measure='PM2.5')]"
            ]
          },
          "execution_count": 173,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "dalipm25row = dalipm25.map(lambda p:\n",
        "        Row(\n",
        "        date = p[0],\n",
        "        location = p[1],\n",
        "        measure = p[2],\n",
        "        hr_01 = float(p[3]), hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n",
        "        hr_06 = float(p[8]),hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n",
        "        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n",
        "        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n",
        "        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n",
        "    )\n",
        ")\n",
        "\n",
        "dalipm25row.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "kF182dzHZ2BU",
        "outputId": "e1eb45e7-1e59-4674-d525-c6c6b9507efe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 53.0| 55.0| 58.0| 53.0| 43.0| 36.0| 35.0| 42.0| 55.0| 64.0| 65.0| 59.0| 52.0| 44.0| 47.0| 41.0| 43.0| 40.0| 42.0| 35.0| 28.0| 20.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/02| 21.0| 22.0| 26.0| 23.0| 20.0| 18.0| 15.0| 21.0| 21.0| 25.0| 29.0| 32.0| 34.0| 29.0| 32.0| 39.0| 51.0| 51.0| 47.0| 43.0| 43.0| 48.0| 47.0| 53.0|      大里|  PM2.5|\n",
            "|2015/01/03| 48.0| 48.0| 43.0| 38.0| 37.0| 36.0| 37.0| 34.0| 37.0| 46.0| 64.0| 77.0| 83.0| 75.0| 68.0| 69.0| 64.0| 65.0| 59.0| 66.0| 71.0| 66.0| 57.0| 48.0|      大里|  PM2.5|\n",
            "|2015/01/04| 60.0| 56.0| 53.0| 43.0| 53.0| 53.0| 52.0| 44.0| 44.0| 50.0| 49.0| 51.0| 45.0| 42.0| 40.0| 38.0| 36.0| 43.0| 51.0| 63.0| 68.0| 72.0| 66.0| 58.0|      大里|  PM2.5|\n",
            "|2015/01/05| 48.0| 42.0| 42.0| 34.0| 34.0| 28.0| 34.0| 35.0| 45.0| 47.0| 54.0| 46.0| 35.0| 19.0| 16.0| 21.0| 24.0| 28.0| 37.0| 52.0| 60.0| 62.0| 64.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/06| 59.0| 40.0| 34.0| 25.0| 27.0| 29.0| 26.0| 33.0| 42.0| 47.0| 38.0| 24.0| 14.0|  8.0| 17.0| 30.0| 51.0| 62.0| 68.0| 83.0| 83.0| 96.0|103.0|110.0|      大里|  PM2.5|\n",
            "|2015/01/08|  7.0|  9.0| 13.0| 18.0| 11.0| 12.0| 17.0| 29.0| 34.0| 39.0| 41.0| 46.0| 46.0| 44.0| 43.0| 39.0| 41.0| 46.0| 47.0| 48.0| 47.0| 47.0| 43.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/09| 35.0| 34.0| 37.0| 30.0| 25.0| 25.0| 22.0| 21.0| 18.0| 20.0| 14.0| 12.0| 21.0| 31.0| 44.0| 46.0| 52.0| 44.0| 39.0| 37.0| 43.0| 43.0| 42.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/10| 38.0| 33.0| 31.0| 24.0| 20.0| 19.0| 22.0| 31.0| 31.0| 45.0| 48.0| 49.0| 38.0| 39.0| 43.0| 46.0| 43.0| 36.0| 33.0| 29.0| 37.0| 34.0| 39.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/11| 37.0| 41.0| 43.0| 43.0| 27.0| 22.0| 26.0| 34.0| 39.0| 37.0| 51.0| 53.0| 61.0| 56.0| 48.0| 43.0| 37.0| 43.0| 43.0| 48.0| 54.0| 51.0| 46.0| 35.0|      大里|  PM2.5|\n",
            "|2015/01/12| 36.0| 40.0| 33.0| 32.0| 33.0| 40.0| 37.0| 34.0| 39.0| 53.0| 60.0| 65.0| 57.0| 50.0| 52.0| 51.0| 43.0| 24.0| 20.0| 28.0| 35.0| 40.0| 30.0| 36.0|      大里|  PM2.5|\n",
            "|2015/01/13| 36.0| 36.0| 32.0| 33.0| 38.0| 45.0| 38.0| 45.0| 45.0| 76.0| 84.0| 96.0| 92.0| 87.0| 64.0| 33.0| 21.0| 22.0| 20.0| 15.0|  7.0| 12.0|  9.0| 11.0|      大里|  PM2.5|\n",
            "|2015/01/14| 10.0|  7.0|  3.0|  0.0|  3.0|  7.0|  5.0|  1.0|  0.0|  0.0|  0.0|  0.0|  0.0|  0.0|  4.0| 12.0| 13.0| 10.0| 12.0| 14.0| 21.0| 19.0| 15.0|  8.0|      大里|  PM2.5|\n",
            "|2015/01/15|  2.0|  3.0|  7.0|  3.0|  7.0|  5.0| 10.0|  7.0| 13.0| 16.0| 14.0|  8.0|  5.0| 13.0| 20.0| 30.0| 30.0| 33.0| 28.0| 29.0| 33.0| 26.0| 23.0| 12.0|      大里|  PM2.5|\n",
            "|2015/01/17| 42.0| 33.0| 25.0| 18.0| 13.0|  9.0| 12.0| 20.0| 28.0| 33.0| 33.0| 43.0| 52.0| 55.0| 57.0| 60.0| 66.0| 77.0| 76.0| 76.0| 77.0| 74.0| 82.0| 75.0|      大里|  PM2.5|\n",
            "|2015/01/18| 77.0| 66.0| 61.0| 62.0| 70.0| 71.0| 69.0| 71.0| 75.0| 82.0| 90.0| 94.0| 88.0| 75.0| 57.0| 47.0| 33.0| 31.0| 25.0| 21.0| 16.0| 16.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/19| 14.0| 14.0| 19.0| 18.0| 16.0|  7.0|  6.0| 12.0| 18.0| 30.0| 26.0| 28.0| 31.0| 32.0| 35.0| 37.0| 41.0| 43.0| 42.0| 50.0| 53.0| 54.0| 57.0| 62.0|      大里|  PM2.5|\n",
            "|2015/01/20| 62.0| 57.0| 52.0| 53.0| 61.0| 62.0| 62.0| 65.0| 73.0| 82.0| 83.0| 88.0| 83.0| 81.0| 73.0| 71.0| 69.0| 71.0| 66.0| 55.0| 49.0| 42.0| 49.0| 43.0|      大里|  PM2.5|\n",
            "|2015/01/21| 46.0| 37.0| 30.0| 31.0| 31.0| 38.0| 42.0| 46.0| 46.0| 36.0| 33.0| 33.0| 37.0| 34.0| 33.0| 33.0| 34.0| 39.0| 48.0| 50.0| 47.0| 45.0| 43.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/23| 61.0| 47.0| 46.0| 55.0| 49.0| 39.0| 29.0| 31.0| 29.0| 19.0| 12.0| 14.0| 30.0| 49.0| 57.0| 55.0| 57.0| 56.0| 64.0| 72.0| 73.0| 75.0| 68.0| 68.0|      大里|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTLug-b4zwB"
      },
      "source": [
        "# 直接透過rdd生成data frame. toDF()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "OpycWwEed5PR",
        "outputId": "63254b9c-d79b-4bcd-83f9-d62f2681e939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|        _1| _2|   _3| _4| _5| _6| _7| _8| _9|_10|_11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|_25|_26|_27|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|2015/01/01| 大里|PM2.5| 53| 55| 58| 53| 43| 36| 35| 42| 55| 64| 65| 59| 52| 44| 47| 41| 43| 40| 42| 35| 28| 20| 18| 16|\n",
            "|2015/01/02| 大里|PM2.5| 21| 22| 26| 23| 20| 18| 15| 21| 21| 25| 29| 32| 34| 29| 32| 39| 51| 51| 47| 43| 43| 48| 47| 53|\n",
            "|2015/01/03| 大里|PM2.5| 48| 48| 43| 38| 37| 36| 37| 34| 37| 46| 64| 77| 83| 75| 68| 69| 64| 65| 59| 66| 71| 66| 57| 48|\n",
            "|2015/01/04| 大里|PM2.5| 60| 56| 53| 43| 53| 53| 52| 44| 44| 50| 49| 51| 45| 42| 40| 38| 36| 43| 51| 63| 68| 72| 66| 58|\n",
            "|2015/01/05| 大里|PM2.5| 48| 42| 42| 34| 34| 28| 34| 35| 45| 47| 54| 46| 35| 19| 16| 21| 24| 28| 37| 52| 60| 62| 64| 61|\n",
            "|2015/01/06| 大里|PM2.5| 59| 40| 34| 25| 27| 29| 26| 33| 42| 47| 38| 24| 14|  8| 17| 30| 51| 62| 68| 83| 83| 96|103|110|\n",
            "|2015/01/08| 大里|PM2.5|  7|  9| 13| 18| 11| 12| 17| 29| 34| 39| 41| 46| 46| 44| 43| 39| 41| 46| 47| 48| 47| 47| 43| 33|\n",
            "|2015/01/09| 大里|PM2.5| 35| 34| 37| 30| 25| 25| 22| 21| 18| 20| 14| 12| 21| 31| 44| 46| 52| 44| 39| 37| 43| 43| 42| 39|\n",
            "|2015/01/10| 大里|PM2.5| 38| 33| 31| 24| 20| 19| 22| 31| 31| 45| 48| 49| 38| 39| 43| 46| 43| 36| 33| 29| 37| 34| 39| 33|\n",
            "|2015/01/11| 大里|PM2.5| 37| 41| 43| 43| 27| 22| 26| 34| 39| 37| 51| 53| 61| 56| 48| 43| 37| 43| 43| 48| 54| 51| 46| 35|\n",
            "|2015/01/12| 大里|PM2.5| 36| 40| 33| 32| 33| 40| 37| 34| 39| 53| 60| 65| 57| 50| 52| 51| 43| 24| 20| 28| 35| 40| 30| 36|\n",
            "|2015/01/13| 大里|PM2.5| 36| 36| 32| 33| 38| 45| 38| 45| 45| 76| 84| 96| 92| 87| 64| 33| 21| 22| 20| 15|  7| 12|  9| 11|\n",
            "|2015/01/14| 大里|PM2.5| 10|  7|  3|  0|  3|  7|  5|  1|  0|  0|  0|  0|  0|  0|  4| 12| 13| 10| 12| 14| 21| 19| 15|  8|\n",
            "|2015/01/15| 大里|PM2.5|  2|  3|  7|  3|  7|  5| 10|  7| 13| 16| 14|  8|  5| 13| 20| 30| 30| 33| 28| 29| 33| 26| 23| 12|\n",
            "|2015/01/17| 大里|PM2.5| 42| 33| 25| 18| 13|  9| 12| 20| 28| 33| 33| 43| 52| 55| 57| 60| 66| 77| 76| 76| 77| 74| 82| 75|\n",
            "|2015/01/18| 大里|PM2.5| 77| 66| 61| 62| 70| 71| 69| 71| 75| 82| 90| 94| 88| 75| 57| 47| 33| 31| 25| 21| 16| 16| 18| 16|\n",
            "|2015/01/19| 大里|PM2.5| 14| 14| 19| 18| 16|  7|  6| 12| 18| 30| 26| 28| 31| 32| 35| 37| 41| 43| 42| 50| 53| 54| 57| 62|\n",
            "|2015/01/20| 大里|PM2.5| 62| 57| 52| 53| 61| 62| 62| 65| 73| 82| 83| 88| 83| 81| 73| 71| 69| 71| 66| 55| 49| 42| 49| 43|\n",
            "|2015/01/21| 大里|PM2.5| 46| 37| 30| 31| 31| 38| 42| 46| 46| 36| 33| 33| 37| 34| 33| 33| 34| 39| 48| 50| 47| 45| 43| 39|\n",
            "|2015/01/23| 大里|PM2.5| 61| 47| 46| 55| 49| 39| 29| 31| 29| 19| 12| 14| 30| 49| 57| 55| 57| 56| 64| 72| 73| 75| 68| 68|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dalipm25.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "1Bku4vR-hfcT",
        "outputId": "4d0ddc85-a1ff-4334-dfd8-e49bb30f9703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|        日期| 測站|   測項| 00| 01| 02| 03| 04| 05| 06| 07| 08| 09| 10| 11| 12| 13| 14| 15| 16| 17| 18| 19| 20| 21| 22| 23|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|2015/01/01| 大里|PM2.5| 53| 55| 58| 53| 43| 36| 35| 42| 55| 64| 65| 59| 52| 44| 47| 41| 43| 40| 42| 35| 28| 20| 18| 16|\n",
            "|2015/01/02| 大里|PM2.5| 21| 22| 26| 23| 20| 18| 15| 21| 21| 25| 29| 32| 34| 29| 32| 39| 51| 51| 47| 43| 43| 48| 47| 53|\n",
            "|2015/01/03| 大里|PM2.5| 48| 48| 43| 38| 37| 36| 37| 34| 37| 46| 64| 77| 83| 75| 68| 69| 64| 65| 59| 66| 71| 66| 57| 48|\n",
            "|2015/01/04| 大里|PM2.5| 60| 56| 53| 43| 53| 53| 52| 44| 44| 50| 49| 51| 45| 42| 40| 38| 36| 43| 51| 63| 68| 72| 66| 58|\n",
            "|2015/01/05| 大里|PM2.5| 48| 42| 42| 34| 34| 28| 34| 35| 45| 47| 54| 46| 35| 19| 16| 21| 24| 28| 37| 52| 60| 62| 64| 61|\n",
            "|2015/01/06| 大里|PM2.5| 59| 40| 34| 25| 27| 29| 26| 33| 42| 47| 38| 24| 14|  8| 17| 30| 51| 62| 68| 83| 83| 96|103|110|\n",
            "|2015/01/08| 大里|PM2.5|  7|  9| 13| 18| 11| 12| 17| 29| 34| 39| 41| 46| 46| 44| 43| 39| 41| 46| 47| 48| 47| 47| 43| 33|\n",
            "|2015/01/09| 大里|PM2.5| 35| 34| 37| 30| 25| 25| 22| 21| 18| 20| 14| 12| 21| 31| 44| 46| 52| 44| 39| 37| 43| 43| 42| 39|\n",
            "|2015/01/10| 大里|PM2.5| 38| 33| 31| 24| 20| 19| 22| 31| 31| 45| 48| 49| 38| 39| 43| 46| 43| 36| 33| 29| 37| 34| 39| 33|\n",
            "|2015/01/11| 大里|PM2.5| 37| 41| 43| 43| 27| 22| 26| 34| 39| 37| 51| 53| 61| 56| 48| 43| 37| 43| 43| 48| 54| 51| 46| 35|\n",
            "|2015/01/12| 大里|PM2.5| 36| 40| 33| 32| 33| 40| 37| 34| 39| 53| 60| 65| 57| 50| 52| 51| 43| 24| 20| 28| 35| 40| 30| 36|\n",
            "|2015/01/13| 大里|PM2.5| 36| 36| 32| 33| 38| 45| 38| 45| 45| 76| 84| 96| 92| 87| 64| 33| 21| 22| 20| 15|  7| 12|  9| 11|\n",
            "|2015/01/14| 大里|PM2.5| 10|  7|  3|  0|  3|  7|  5|  1|  0|  0|  0|  0|  0|  0|  4| 12| 13| 10| 12| 14| 21| 19| 15|  8|\n",
            "|2015/01/15| 大里|PM2.5|  2|  3|  7|  3|  7|  5| 10|  7| 13| 16| 14|  8|  5| 13| 20| 30| 30| 33| 28| 29| 33| 26| 23| 12|\n",
            "|2015/01/17| 大里|PM2.5| 42| 33| 25| 18| 13|  9| 12| 20| 28| 33| 33| 43| 52| 55| 57| 60| 66| 77| 76| 76| 77| 74| 82| 75|\n",
            "|2015/01/18| 大里|PM2.5| 77| 66| 61| 62| 70| 71| 69| 71| 75| 82| 90| 94| 88| 75| 57| 47| 33| 31| 25| 21| 16| 16| 18| 16|\n",
            "|2015/01/19| 大里|PM2.5| 14| 14| 19| 18| 16|  7|  6| 12| 18| 30| 26| 28| 31| 32| 35| 37| 41| 43| 42| 50| 53| 54| 57| 62|\n",
            "|2015/01/20| 大里|PM2.5| 62| 57| 52| 53| 61| 62| 62| 65| 73| 82| 83| 88| 83| 81| 73| 71| 69| 71| 66| 55| 49| 42| 49| 43|\n",
            "|2015/01/21| 大里|PM2.5| 46| 37| 30| 31| 31| 38| 42| 46| 46| 36| 33| 33| 37| 34| 33| 33| 34| 39| 48| 50| 47| 45| 43| 39|\n",
            "|2015/01/23| 大里|PM2.5| 61| 47| 46| 55| 49| 39| 29| 31| 29| 19| 12| 14| 30| 49| 57| 55| 57| 56| 64| 72| 73| 75| 68| 68|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pm25schema = weather_data_rdd.first()\n",
        "dalipm25.toDF(pm25schema).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByodqKk3knMK"
      },
      "outputs": [],
      "source": [
        "dfpm25 = dalipm25.toDF(pm25schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "np4IHhqFyJ8Z"
      },
      "outputs": [],
      "source": [
        "for i in dfpm25.columns[3:]:\n",
        "  dfpm25 = dfpm25.withColumn(i, dfpm25[i].cast(\"double\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "kYRCksyPll5S",
        "outputId": "12702fea-1e1f-49e1-d9e4-1054b232627b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- 日期: string (nullable = true)\n",
            " |-- 測站: string (nullable = true)\n",
            " |-- 測項: string (nullable = true)\n",
            " |-- 00: double (nullable = true)\n",
            " |-- 01: double (nullable = true)\n",
            " |-- 02: double (nullable = true)\n",
            " |-- 03: double (nullable = true)\n",
            " |-- 04: double (nullable = true)\n",
            " |-- 05: double (nullable = true)\n",
            " |-- 06: double (nullable = true)\n",
            " |-- 07: double (nullable = true)\n",
            " |-- 08: double (nullable = true)\n",
            " |-- 09: double (nullable = true)\n",
            " |-- 10: double (nullable = true)\n",
            " |-- 11: double (nullable = true)\n",
            " |-- 12: double (nullable = true)\n",
            " |-- 13: double (nullable = true)\n",
            " |-- 14: double (nullable = true)\n",
            " |-- 15: double (nullable = true)\n",
            " |-- 16: double (nullable = true)\n",
            " |-- 17: double (nullable = true)\n",
            " |-- 18: double (nullable = true)\n",
            " |-- 19: double (nullable = true)\n",
            " |-- 20: double (nullable = true)\n",
            " |-- 21: double (nullable = true)\n",
            " |-- 22: double (nullable = true)\n",
            " |-- 23: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfpm25.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkGg9xJWZ2Bb"
      },
      "source": [
        "# 使用 DataFrame.filter() 來進行row資料之條件計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vtr4GV5bZ2Ba",
        "outputId": "ad5a0d28-eeea-475a-9430-778a9b1f692a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 53.0| 55.0| 58.0| 53.0| 43.0| 36.0| 35.0| 42.0| 55.0| 64.0| 65.0| 59.0| 52.0| 44.0| 47.0| 41.0| 43.0| 40.0| 42.0| 35.0| 28.0| 20.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/03| 48.0| 48.0| 43.0| 38.0| 37.0| 36.0| 37.0| 34.0| 37.0| 46.0| 64.0| 77.0| 83.0| 75.0| 68.0| 69.0| 64.0| 65.0| 59.0| 66.0| 71.0| 66.0| 57.0| 48.0|      大里|  PM2.5|\n",
            "|2015/01/04| 60.0| 56.0| 53.0| 43.0| 53.0| 53.0| 52.0| 44.0| 44.0| 50.0| 49.0| 51.0| 45.0| 42.0| 40.0| 38.0| 36.0| 43.0| 51.0| 63.0| 68.0| 72.0| 66.0| 58.0|      大里|  PM2.5|\n",
            "|2015/01/05| 48.0| 42.0| 42.0| 34.0| 34.0| 28.0| 34.0| 35.0| 45.0| 47.0| 54.0| 46.0| 35.0| 19.0| 16.0| 21.0| 24.0| 28.0| 37.0| 52.0| 60.0| 62.0| 64.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/06| 59.0| 40.0| 34.0| 25.0| 27.0| 29.0| 26.0| 33.0| 42.0| 47.0| 38.0| 24.0| 14.0|  8.0| 17.0| 30.0| 51.0| 62.0| 68.0| 83.0| 83.0| 96.0|103.0|110.0|      大里|  PM2.5|\n",
            "|2015/01/09| 35.0| 34.0| 37.0| 30.0| 25.0| 25.0| 22.0| 21.0| 18.0| 20.0| 14.0| 12.0| 21.0| 31.0| 44.0| 46.0| 52.0| 44.0| 39.0| 37.0| 43.0| 43.0| 42.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/10| 38.0| 33.0| 31.0| 24.0| 20.0| 19.0| 22.0| 31.0| 31.0| 45.0| 48.0| 49.0| 38.0| 39.0| 43.0| 46.0| 43.0| 36.0| 33.0| 29.0| 37.0| 34.0| 39.0| 33.0|      大里|  PM2.5|\n",
            "|2015/01/11| 37.0| 41.0| 43.0| 43.0| 27.0| 22.0| 26.0| 34.0| 39.0| 37.0| 51.0| 53.0| 61.0| 56.0| 48.0| 43.0| 37.0| 43.0| 43.0| 48.0| 54.0| 51.0| 46.0| 35.0|      大里|  PM2.5|\n",
            "|2015/01/12| 36.0| 40.0| 33.0| 32.0| 33.0| 40.0| 37.0| 34.0| 39.0| 53.0| 60.0| 65.0| 57.0| 50.0| 52.0| 51.0| 43.0| 24.0| 20.0| 28.0| 35.0| 40.0| 30.0| 36.0|      大里|  PM2.5|\n",
            "|2015/01/13| 36.0| 36.0| 32.0| 33.0| 38.0| 45.0| 38.0| 45.0| 45.0| 76.0| 84.0| 96.0| 92.0| 87.0| 64.0| 33.0| 21.0| 22.0| 20.0| 15.0|  7.0| 12.0|  9.0| 11.0|      大里|  PM2.5|\n",
            "|2015/01/17| 42.0| 33.0| 25.0| 18.0| 13.0|  9.0| 12.0| 20.0| 28.0| 33.0| 33.0| 43.0| 52.0| 55.0| 57.0| 60.0| 66.0| 77.0| 76.0| 76.0| 77.0| 74.0| 82.0| 75.0|      大里|  PM2.5|\n",
            "|2015/01/18| 77.0| 66.0| 61.0| 62.0| 70.0| 71.0| 69.0| 71.0| 75.0| 82.0| 90.0| 94.0| 88.0| 75.0| 57.0| 47.0| 33.0| 31.0| 25.0| 21.0| 16.0| 16.0| 18.0| 16.0|      大里|  PM2.5|\n",
            "|2015/01/20| 62.0| 57.0| 52.0| 53.0| 61.0| 62.0| 62.0| 65.0| 73.0| 82.0| 83.0| 88.0| 83.0| 81.0| 73.0| 71.0| 69.0| 71.0| 66.0| 55.0| 49.0| 42.0| 49.0| 43.0|      大里|  PM2.5|\n",
            "|2015/01/21| 46.0| 37.0| 30.0| 31.0| 31.0| 38.0| 42.0| 46.0| 46.0| 36.0| 33.0| 33.0| 37.0| 34.0| 33.0| 33.0| 34.0| 39.0| 48.0| 50.0| 47.0| 45.0| 43.0| 39.0|      大里|  PM2.5|\n",
            "|2015/01/23| 61.0| 47.0| 46.0| 55.0| 49.0| 39.0| 29.0| 31.0| 29.0| 19.0| 12.0| 14.0| 30.0| 49.0| 57.0| 55.0| 57.0| 56.0| 64.0| 72.0| 73.0| 75.0| 68.0| 68.0|      大里|  PM2.5|\n",
            "|2015/01/24| 63.0| 61.0| 59.0| 58.0| 56.0| 52.0| 39.0| 33.0| 34.0| 40.0| 30.0| 33.0| 49.0| 66.0| 76.0| 75.0| 73.0| 60.0| 58.0| 67.0| 83.0| 88.0| 87.0| 80.0|      大里|  PM2.5|\n",
            "|2015/01/25| 71.0| 68.0| 65.0| 63.0| 63.0| 62.0| 57.0| 55.0| 65.0| 70.0| 79.0| 75.0| 75.0| 64.0| 59.0| 68.0| 78.0| 86.0| 80.0| 74.0| 66.0| 60.0| 49.0| 44.0|      大里|  PM2.5|\n",
            "|2015/01/26| 50.0| 53.0| 48.0| 43.0| 38.0| 40.0| 38.0| 47.0| 49.0| 59.0| 55.0| 53.0| 39.0| 29.0| 28.0| 38.0| 51.0| 55.0| 53.0| 55.0| 54.0| 61.0| 55.0| 61.0|      大里|  PM2.5|\n",
            "|2015/01/27| 56.0| 52.0| 41.0| 46.0| 49.0| 52.0| 43.0| 43.0| 48.0| 48.0| 40.0| 30.0| 30.0| 33.0| 36.0| 37.0| 35.0| 36.0| 43.0| 46.0| 42.0| 30.0| 22.0| 17.0|      大里|  PM2.5|\n",
            "|2015/02/07| 32.0| 32.0| 38.0| 43.0| 49.0| 48.0| 47.0| 54.0| 56.0| 60.0| 62.0| 66.0| 70.0| 73.0| 60.0| 46.0| 23.0| 25.0| 23.0| 40.0| 36.0| 32.0| 25.0| 29.0|      大里|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.hr_01>30).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOULjFJZ2Be"
      },
      "source": [
        "# 使用 DataFrame.select() 來進行資料之Projection http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCXgch-_Z2Bc",
        "outputId": "4dd2dc5e-e342-4797-f7aa-401ddc610df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------+-------+\n",
            "|hr_01|location|measure|\n",
            "+-----+--------+-------+\n",
            "| 53.0|      大里|  PM2.5|\n",
            "| 60.0|      大里|  PM2.5|\n",
            "| 59.0|      大里|  PM2.5|\n",
            "| 77.0|      大里|  PM2.5|\n",
            "| 62.0|      大里|  PM2.5|\n",
            "| 61.0|      大里|  PM2.5|\n",
            "| 63.0|      大里|  PM2.5|\n",
            "| 71.0|      大里|  PM2.5|\n",
            "| 56.0|      大里|  PM2.5|\n",
            "| 51.0|      大里|  PM2.5|\n",
            "| 54.0|      大里|  PM2.5|\n",
            "| 66.0|      大里|  PM2.5|\n",
            "| 67.0|      大里|  PM2.5|\n",
            "| 58.0|      大里|  PM2.5|\n",
            "| 52.0|      大里|  PM2.5|\n",
            "| 70.0|      大里|  PM2.5|\n",
            "| 83.0|      大里|  PM2.5|\n",
            "| 52.0|      大里|  PM2.5|\n",
            "| 51.0|      大里|  PM2.5|\n",
            "| 69.0|      大里|  PM2.5|\n",
            "+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.filter(df.hr_01>50).select(\"hr_01\",\"location\",\"measure\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6sInWJZ2Bg"
      },
      "source": [
        "## 使用 DataFrame.describe() 來進行DataFrame or Column 資料之統計 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-vy-Ms1Z2Bh",
        "outputId": "84433339-d42b-4f15-fe2d-b50e7ea133da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "|summary|      date|            hr_01|            hr_02|            hr_03|             hr_04|             hr_05|             hr_06|             hr_07|            hr_08|             hr_09|             hr_10|            hr_11|             hr_12|             hr_13|             hr_14|             hr_15|             hr_16|             hr_17|             hr_18|             hr_19|             hr_20|             hr_21|             hr_22|            hr_23|             hr_24|location|measure|\n",
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "|  count|       198|              198|              198|              198|               198|               198|               198|               198|              198|               198|               198|              198|               198|               198|               198|               198|               198|               198|               198|               198|               198|               198|               198|              198|               198|     198|    198|\n",
            "|   mean|      null|30.04040404040404| 29.1010101010101| 28.1010101010101|27.065656565656564|25.853535353535353|25.055555555555557|24.883838383838384|26.51010101010101|29.636363636363637| 32.80808080808081|34.92424242424242| 35.43939393939394| 35.38383838383838| 34.59090909090909| 33.43434343434343|32.474747474747474|31.166666666666668|30.818181818181817| 30.63131313131313|31.747474747474747|32.505050505050505| 32.63636363636363|32.18181818181818|30.939393939393938|    null|   null|\n",
            "| stddev|      null|20.06533449925237|19.65669565958899|19.77410240385312|19.417683403396772|19.423512953586293|18.827346577025022|17.742708273179396| 17.9863851214172|18.715858009304785|19.908509910497596|19.82561841058815|20.238021803881413|20.277119245283668|20.342481822089674|19.235997143765555|18.886432395137792| 17.98046034768634| 18.04948844775545|17.872090066443693|19.357624678266657| 20.29271767424397|21.098249522238337|21.75118347272709| 21.03336396373215|    null|   null|\n",
            "|    min|2015/01/01|              2.0|              0.0|              0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|      大里|  PM2.5|\n",
            "|    max|2015/12/29|            114.0|            109.0|            107.0|             102.0|              98.0|              88.0|              74.0|             77.0|              86.0|              90.0|             90.0|              99.0|             111.0|             101.0|              94.0|             104.0|              89.0|              86.0|              88.0|              99.0|             101.0|              98.0|            107.0|             110.0|      大里|  PM2.5|\n",
            "+-------+----------+-----------------+-----------------+-----------------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+--------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcSbB_ZZ2BZ"
      },
      "source": [
        "# 使用 DataFrame.agg() 來進行column數值之統計計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UP5lLypj1P8T"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "p-PTxlE0Z2Be",
        "outputId": "4432ceeb-8496-41f7-88a5-604257ecbb94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+----------------+\n",
            "|       avg(hr_01)|      avg(hr_02)|\n",
            "+-----------------+----------------+\n",
            "|30.04040404040404|29.1010101010101|\n",
            "+-----------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "df.agg(F.mean(df.hr_01),F.mean(df.hr_02)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jtfh2MQHTY"
      },
      "source": [
        "#使用 DataFrame.withColumn() 來進行column新增column http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "gz_KzHSQQbtB",
        "outputId": "6cc0dfe4-4888-4ca8-d3e0-e565be38bb9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[date: string, hr_01: double, hr_02: double, hr_03: double, hr_04: double, hr_05: double, hr_06: double, hr_07: double, hr_08: double, hr_09: double, hr_10: double, hr_11: double, hr_12: double, hr_13: double, hr_14: double, hr_15: double, hr_16: double, hr_17: double, hr_18: double, hr_19: double, hr_20: double, hr_21: double, hr_22: double, hr_23: double, hr_24: double, location: string, measure: string]"
            ]
          },
          "execution_count": 25,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.withColumn('hr_01', df.hr_01+df.hr_02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyEnb1kZ2CD"
      },
      "source": [
        "# 練習1: 請算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kti4CDbZ2CC"
      },
      "source": [
        "# 練習2: 請使用SPARK SQL求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doUg4ZNDN3n4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycnCvcAVZ2Bi"
      },
      "source": [
        "# 使用 Spark SQL來下達SQL查詢"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfO7cTEUZ2Bj"
      },
      "outputs": [],
      "source": [
        "df.registerTempTable(\"DaliTable\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hSsE-3AWZ2Bl",
        "outputId": "4af8a21d-4c3f-4490-895e-0e7eedfd1577"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:An unexpected error occurred while tokenizing input\n",
            "The following traceback may be corrupted or invalid\n",
            "The error message is: ('EOF in multi-line string', (1, 10))\n",
            "\n"
          ]
        },
        {
          "ename": "AnalysisException",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o109.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`date`' given input columns: [_8, _11, _21, _14, _12, _23, _7, _4, _27, _1, _25, _9, _6, _22, _24, _2, _5, _3, _18, _15, _26, _10, _17, _13, _16, _19, _20]; line 2 pos 42;\n'Project [*]\n+- 'Filter ('date = 2015/02/07)\n   +- SubqueryAlias dalitable\n      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\n               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, cast(_24#2671 as double) AS _24#3313, ... 3 more fields]\n                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, cast(_23#2670 as double) AS _23#3284, _24#2671, ... 3 more fields]\n                     +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, cast(_22#2669 as double) AS _22#3255, _23#2670, _24#2671, ... 3 more fields]\n                        +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, cast(_21#2668 as double) AS _21#3226, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                           +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, cast(_20#2667 as double) AS _20#3197, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                              +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, cast(_19#2666 as double) AS _19#3168, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                 +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, cast(_18#2665 as double) AS _18#3139, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                    +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, cast(_17#2664 as double) AS _17#3110, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                       +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, cast(_16#2663 as double) AS _16#3081, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                          +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, cast(_15#2662 as double) AS _15#3052, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                             +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, cast(_14#2661 as double) AS _14#3023, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, cast(_13#2660 as double) AS _13#2994, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                   +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, cast(_12#2659 as double) AS _12#2965, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, cast(_11#2658 as double) AS _11#2936, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, cast(_10#2657 as double) AS _10#2907, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, cast(_9#2656 as double) AS _9#2878, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, cast(_8#2655 as double) AS _8#2849, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, cast(_7#2654 as double) AS _7#2820, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                     +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, cast(_6#2653 as double) AS _6#2791, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                        +- Project [_1#2648, _2#2649, _3#2650, _4#2733, cast(_5#2652 as double) AS _5#2762, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                           +- Project [_1#2648, _2#2649, _3#2650, cast(_4#2651 as double) AS _4#2733, _5#2652, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n                                                                              +- LogicalRDD [_1#2648, _2#2649, _3#2650, _4#2651, _5#2652, _6#2653, _7#2654, _8#2655, _9#2656, _10#2657, _11#2658, _12#2659, _13#2660, _14#2661, _15#2662, _16#2663, _17#2664, _18#2665, _19#2666, _20#2667, _21#2668, _22#2669, _23#2670, _24#2671, ... 3 more fields]\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:88)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:289)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$6.apply(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:298)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:268)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:78)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:52)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:66)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:623)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-fbee181fab89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m spark.sql(\"\"\"\n\u001b[1;32m      2\u001b[0m             \u001b[0mselect\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mDaliTable\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'2015/02/07'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           \"\"\").show()\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \"\"\"\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`date`' given input columns: [_8, _11, _21, _14, _12, _23, _7, _4, _27, _1, _25, _9, _6, _22, _24, _2, _5, _3, _18, _15, _26, _10, _17, _13, _16, _19, _20]; line 2 pos 42;\\n'Project [*]\\n+- 'Filter ('date = 2015/02/07)\\n   +- SubqueryAlias dalitable\\n      +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n         +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n            +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, _24#3313, ... 3 more fields]\\n               +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6#2791, _7#2820, _8#2849, _9#2878, _10#2907, _11#2936, _12#2965, _13#2994, _14#3023, _15#3052, _16#3081, _17#3110, _18#3139, _19#3168, _20#3197, _21#3226, _22#3255, _23#3284, cast(_24#2671 as double) AS _24#3313, ... 3 more fields]\\n                  +- Project [_1#2648, _2#2649, _3#2650, _4#2733, _5#2762, _6..."
          ]
        }
      ],
      "source": [
        "spark.sql(\"\"\"\n",
        "            select * from DaliTable where date ='2015/02/07'\n",
        "          \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66vVEpvUZ2Bo"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "                select count(*) count \n",
        "                from DaliTable \n",
        "                where hr_01 > 100\n",
        "               \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxauml1-Z2Bt"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "                select AVG(hr_01) count \n",
        "                from DaliTable\n",
        "               \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jux8542WZ2Bv"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n",
        "                from DaliTable\n",
        "          \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgoKjn6PZ2Bw"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01+hr_02 as diff \n",
        "                from DaliTable \n",
        "                order by diff DESC\n",
        "                \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dzr5S_ysZ2By"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01+hr_02+hr_03+hr_04+hr_05+hr_06+hr_07+hr_08 as sum \n",
        "                from DaliTable \n",
        "                order by sum DESC\n",
        "                \"\"\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVcomNQfZ2B0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HryYFAWXZ2B1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMpQ-zrB11pp"
      },
      "source": [
        "# 利用DataFrame 直接讀取CSV, Fillna, DropNa功能清洗資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO_EEY1011pq"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pDYpUS811ps"
      },
      "outputs": [],
      "source": [
        "df = spark.read.load(\"./pm25.csv\", format=\"csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBn5I6iY11pt",
        "outputId": "4797fe85-1105-49ed-fcf0-59d11a462d96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|       _c0|_c1|       _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|        日期| 測站|        測項|  00|  01|  02|  03|  04|  05|  06|  07|  08|  09|  10|  11|  12|  13|  14|  15|  16|  17|  18|  19|  20|  21|  22|  23|\n",
            "|2015/01/01| 龍潭|  AMB_TEMP|  14|  14|  14|  13|  13|  13|  12|  12|  13|  14|  14|  14|  14|  14|  13|  13|  12|  11|  11|  11|  11|  11|  11|  11|\n",
            "|2015/01/01| 龍潭|        CO|0.69|0.72|0.69|0.64|0.54|0.47|0.45|0.48|0.51|0.54|0.54| 0.5|0.47|0.38|0.36|0.35|0.34|0.37|0.34|0.29|0.26|0.22|0.19|0.18|\n",
            "|2015/01/01| 龍潭|        NO| 0.3| 0.1| 0.6|   2|   2| 1.9| 2.2| 3.1| 3.7| 4.3| 4.3| 4.5| 3.3| 4.1| 3.1| 3.6| 3.6| 2.8| 2.8| 2.5| 2.2| 1.4| 2.1|   2|\n",
            "|2015/01/01| 龍潭|       NO2|  11| 9.6| 8.7| 9.1| 9.6| 9.9|  11|  13|  11|  12|  12|  11|  11| 9.9| 9.9|  10|  11|  13|  11|  10| 8.2| 7.3| 6.5| 5.5|\n",
            "|2015/01/01| 龍潭|       NOx|  11| 9.7| 9.3|  11|  12|  12|  13|  17|  15|  16|  16|  16|  14|  14|  13|  14|  15|  16|  14|  13|  10| 8.7| 8.6| 7.5|\n",
            "|2015/01/01| 龍潭|        O3|  44|  43|  43|  44|  41|  39|  38|  34|  37|  37|  39|  44|  47|  49|  48|  44|  39|  37|  37|  39|  39|  38|  38|  39|\n",
            "|2015/01/01| 龍潭|      PM10| 106| 138| 152| 152| 143| 128| 115| 106| 102| 105| 108| 114| 108|  96|  82|  74|  72|  72|  70|  60|  50|  37|  38|  40|\n",
            "|2015/01/01| 龍潭|     PM2.5|  46|  71|  76|  74|  65|  62|  56|  50|  52|  56|  54|  47|  40|  36|  37|  27|  30|  25|  26|  24|  18|  16|  11|  14|\n",
            "|2015/01/01| 龍潭|  RAINFALL|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|\n",
            "|2015/01/01| 龍潭|        RH|  70|  69|  70|  70|  71|  72|  70|  69|  64|  60|  57|  53|  52|  52|  53|  56|  59|  60|  63|  62|  60|  62|  65|  64|\n",
            "|2015/01/01| 龍潭|       SO2| 8.6| 8.5| 7.6| 6.7| 7.4| 6.4| 6.2| 7.2| 6.7| 6.5| 5.5| 4.8| 4.1| 3.2| 2.8| 2.8| 3.1| 2.8| 2.5|   2| 1.6| 1.5| 1.4| 1.2|\n",
            "|2015/01/01| 龍潭|     WD_HR|  66|  70|  69|  68|  67|  72|  74|  72|  66|  66|  63|  60|  59|  62|  59|  63|  64|  62|  62|  62|  59|  59|  59|  61|\n",
            "|2015/01/01| 龍潭|WIND_DIREC|  67|  70|  68|  67|  66|  75|  75|  73|  68|  64|  59|  59|  65|  62|  60|  68|  60|  61|  62|  61|  59|  63|  65|  58|\n",
            "|2015/01/01| 龍潭|WIND_SPEED| 8.7| 7.5| 7.7| 7.2| 6.9| 5.3| 6.5| 6.4|   7| 7.1| 7.5| 7.3| 6.9| 7.4|   8| 8.3| 6.6| 7.3| 7.1|   7| 6.9| 7.5| 6.8| 5.8|\n",
            "|2015/01/01| 龍潭|     WS_HR|   8| 7.7| 7.2| 6.9|   7| 6.6| 6.3| 6.2| 7.1|   7| 7.1| 7.6| 7.5| 7.5| 7.9| 7.9| 7.2| 6.9| 7.2| 6.9| 6.7| 6.9| 6.9| 5.7|\n",
            "|2015/01/02| 龍潭|  AMB_TEMP|  11|  11|  11|  11|  11|  11|  11|  11|  13|  14|  15|  16|  16|  16|  16|  16|  15|  14|  13|  13|  13|  12|  12|  12|\n",
            "|2015/01/02| 龍潭|        CO|0.17|0.17|0.18|0.18|0.23|0.24|0.26|0.33|0.34|0.35|0.35|0.34|0.32|0.31| 0.3|0.32| 0.3|0.34|0.33| 0.3|0.28|0.28|0.27|0.26|\n",
            "|2015/01/02| 龍潭|        NO| 2.2| 1.6| 1.7| 1.8|   2| 1.9| 2.5| 3.4| 4.5|   5| 4.6| 4.6| 4.3|   4|   4| 3.6| 3.6| 3.1| 2.7| 2.6| 2.9| 2.3| 1.9| 2.2|\n",
            "|2015/01/02| 龍潭|       NO2|   5| 5.4| 5.1| 5.1| 5.6|   7| 7.5| 9.6| 9.7| 9.5| 7.8| 7.4| 7.1| 7.1| 7.9| 9.2|  11|  13|  12|  10| 9.2| 8.5| 8.8| 9.2|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2Du-yrk11pw"
      },
      "outputs": [],
      "source": [
        "df2=df.filter(df[\"_c0\"]!=\"日期\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eRUGtKK11py",
        "outputId": "8cd77220-d96a-4487-e700-1caad78fbd85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "465317"
            ]
          },
          "execution_count": 143,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcL55Mg211p0",
        "outputId": "9a74cfa8-3d4c-4815-cb83-555477c6b28e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string, _c21: string, _c22: string, _c23: string, _c24: string, _c25: string, _c26: string]"
            ]
          },
          "execution_count": 144,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CN4CwP_11p2",
        "outputId": "ef4f82d1-8d77-4a83-8fd0-e6bf3e319165"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "465317"
            ]
          },
          "execution_count": 145,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOg8NZWA11p3",
        "outputId": "1074ebd2-6695-47a3-a420-a76998f788ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|       _c0|_c1|       _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "|2015/01/01| 龍潭|  AMB_TEMP|  14|  14|  14|  13|  13|  13|  12|  12|  13|  14|  14|  14|  14|  14|  13|  13|  12|  11|  11|  11|  11|  11|  11|  11|\n",
            "|2015/01/01| 龍潭|        CO|0.69|0.72|0.69|0.64|0.54|0.47|0.45|0.48|0.51|0.54|0.54| 0.5|0.47|0.38|0.36|0.35|0.34|0.37|0.34|0.29|0.26|0.22|0.19|0.18|\n",
            "|2015/01/01| 龍潭|        NO| 0.3| 0.1| 0.6|   2|   2| 1.9| 2.2| 3.1| 3.7| 4.3| 4.3| 4.5| 3.3| 4.1| 3.1| 3.6| 3.6| 2.8| 2.8| 2.5| 2.2| 1.4| 2.1|   2|\n",
            "|2015/01/01| 龍潭|       NO2|  11| 9.6| 8.7| 9.1| 9.6| 9.9|  11|  13|  11|  12|  12|  11|  11| 9.9| 9.9|  10|  11|  13|  11|  10| 8.2| 7.3| 6.5| 5.5|\n",
            "|2015/01/01| 龍潭|       NOx|  11| 9.7| 9.3|  11|  12|  12|  13|  17|  15|  16|  16|  16|  14|  14|  13|  14|  15|  16|  14|  13|  10| 8.7| 8.6| 7.5|\n",
            "|2015/01/01| 龍潭|        O3|  44|  43|  43|  44|  41|  39|  38|  34|  37|  37|  39|  44|  47|  49|  48|  44|  39|  37|  37|  39|  39|  38|  38|  39|\n",
            "|2015/01/01| 龍潭|      PM10| 106| 138| 152| 152| 143| 128| 115| 106| 102| 105| 108| 114| 108|  96|  82|  74|  72|  72|  70|  60|  50|  37|  38|  40|\n",
            "|2015/01/01| 龍潭|     PM2.5|  46|  71|  76|  74|  65|  62|  56|  50|  52|  56|  54|  47|  40|  36|  37|  27|  30|  25|  26|  24|  18|  16|  11|  14|\n",
            "|2015/01/01| 龍潭|  RAINFALL|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|  NR|\n",
            "|2015/01/01| 龍潭|        RH|  70|  69|  70|  70|  71|  72|  70|  69|  64|  60|  57|  53|  52|  52|  53|  56|  59|  60|  63|  62|  60|  62|  65|  64|\n",
            "|2015/01/01| 龍潭|       SO2| 8.6| 8.5| 7.6| 6.7| 7.4| 6.4| 6.2| 7.2| 6.7| 6.5| 5.5| 4.8| 4.1| 3.2| 2.8| 2.8| 3.1| 2.8| 2.5|   2| 1.6| 1.5| 1.4| 1.2|\n",
            "|2015/01/01| 龍潭|     WD_HR|  66|  70|  69|  68|  67|  72|  74|  72|  66|  66|  63|  60|  59|  62|  59|  63|  64|  62|  62|  62|  59|  59|  59|  61|\n",
            "|2015/01/01| 龍潭|WIND_DIREC|  67|  70|  68|  67|  66|  75|  75|  73|  68|  64|  59|  59|  65|  62|  60|  68|  60|  61|  62|  61|  59|  63|  65|  58|\n",
            "|2015/01/01| 龍潭|WIND_SPEED| 8.7| 7.5| 7.7| 7.2| 6.9| 5.3| 6.5| 6.4|   7| 7.1| 7.5| 7.3| 6.9| 7.4|   8| 8.3| 6.6| 7.3| 7.1|   7| 6.9| 7.5| 6.8| 5.8|\n",
            "|2015/01/01| 龍潭|     WS_HR|   8| 7.7| 7.2| 6.9|   7| 6.6| 6.3| 6.2| 7.1|   7| 7.1| 7.6| 7.5| 7.5| 7.9| 7.9| 7.2| 6.9| 7.2| 6.9| 6.7| 6.9| 6.9| 5.7|\n",
            "|2015/01/02| 龍潭|  AMB_TEMP|  11|  11|  11|  11|  11|  11|  11|  11|  13|  14|  15|  16|  16|  16|  16|  16|  15|  14|  13|  13|  13|  12|  12|  12|\n",
            "|2015/01/02| 龍潭|        CO|0.17|0.17|0.18|0.18|0.23|0.24|0.26|0.33|0.34|0.35|0.35|0.34|0.32|0.31| 0.3|0.32| 0.3|0.34|0.33| 0.3|0.28|0.28|0.27|0.26|\n",
            "|2015/01/02| 龍潭|        NO| 2.2| 1.6| 1.7| 1.8|   2| 1.9| 2.5| 3.4| 4.5|   5| 4.6| 4.6| 4.3|   4|   4| 3.6| 3.6| 3.1| 2.7| 2.6| 2.9| 2.3| 1.9| 2.2|\n",
            "|2015/01/02| 龍潭|       NO2|   5| 5.4| 5.1| 5.1| 5.6|   7| 7.5| 9.6| 9.7| 9.5| 7.8| 7.4| 7.1| 7.1| 7.9| 9.2|  11|  13|  12|  10| 9.2| 8.5| 8.8| 9.2|\n",
            "|2015/01/02| 龍潭|       NOx| 7.2|   7| 6.8| 6.9| 7.6|   9|  10|  13|  14|  14|  12|  12|  11|  11|  12|  13|  14|  16|  15|  13|  12|  11|  11|  11|\n",
            "+----------+---+----------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2.filter(df2._c3.isNotNull()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeL-EKam11p5"
      },
      "outputs": [],
      "source": [
        "for i in df2.columns[3:]:\n",
        "  df2 = df2.withColumn(i, df2[i].cast(\"double\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCw-zU4211p6",
        "outputId": "931e2706-f384-4e3d-e49b-36960d8fec74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "366789"
            ]
          },
          "execution_count": 148,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2.dropna().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6GLXSxK11p7"
      },
      "outputs": [],
      "source": [
        "df2 = df2.filter(df2[\"_c2\"]!=\"RAINFALL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUx3JyvT11p9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coU6Cj_S11p_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbM7XshZ2B2"
      },
      "source": [
        "# Perform Pearson Correlation using DataFrame.corr( )\n",
        "# 練習: 計算大里區pm10, pm2.5 間之關聯度 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
        "使用前面所建立的clean_weather_data rdd資料\n",
        "    \n",
        "    \n",
        "    corr(col1, col2, method=None)\n",
        "    Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
        "\n",
        "    Parameters:\t\n",
        "    col1 – The name of the first column\n",
        "    col2 – The name of the second column\n",
        "    method – The correlation method. Currently only supports “pearson”\n",
        "    New in version 1.4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nW5zsuLAZ2B3"
      },
      "outputs": [],
      "source": [
        "def Generated_Measurement(x):\n",
        "    date = x[0]\n",
        "    location = x[1]\n",
        "    measure = x[2]\n",
        "    measurements_of_a_day = []\n",
        "    for i, value in enumerate(x[3:]):\n",
        "        measurements_of_a_day.append((date, measure, \"hr_\"+str(i), value))\n",
        "    return measurements_of_a_day\n",
        "\n",
        "daliData = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\n",
        "daliData.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y8sIyUD5sBg"
      },
      "outputs": [],
      "source": [
        "daliData.toDF().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49larQ_8Z2B4"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "daliDataRow = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\\\n",
        "    .flatMap(Generated_Measurement)\\\n",
        "    .map(lambda x: ( (x[0], x[2]), x[1], x[3] ) )\\\n",
        "    .groupBy(lambda x: x[0])\\\n",
        "    .filter(lambda x: len(x[1])==2)\\\n",
        "    .mapValues(lambda x: list(x))\\\n",
        "    .mapValues(lambda x: [x[0][1], x[0][2], x[1][1], x[1][2]])\\\n",
        "    .map(lambda x:[ x[0][0], x[0][1], x[1][1], x[1][3]])\\\n",
        "    .map(lambda x: Row(\n",
        "            date = x[0],\n",
        "            time = x[1],\n",
        "            pm10 = float(x[2]),\n",
        "            pm25 = float(x[3])\n",
        "        ))\n",
        "    \n",
        "df = spark.createDataFrame(daliDataRow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0vjzrg0Z2B6"
      },
      "outputs": [],
      "source": [
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7OPjFeNZ2B7"
      },
      "outputs": [],
      "source": [
        "df.groupBy().avg().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOCH_rb-Z2B-"
      },
      "outputs": [],
      "source": [
        "df.corr(\"pm10\",\"pm25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "AVaWYyQ_Z2CA",
        "outputId": "ece851c9-d481-46b7-e4a3-450c92ed1f05"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-dde026cdb0e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdescribe\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m             \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df.describe().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FukeJJoY11Xq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9FisGymJZ2BM",
        "ZQTLug-b4zwB",
        "mkGg9xJWZ2Bb",
        "xIOULjFJZ2Be",
        "CEcSbB_ZZ2BZ",
        "ycnCvcAVZ2Bi",
        "pMpQ-zrB11pp",
        "0WbM7XshZ2B2"
      ],
      "include_colab_link": true,
      "name": "2019 Spark DataFrame Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
