{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "(Spark Tutorial) 上課講義 2019_Spark_DataFrame_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9FisGymJZ2BM",
        "ZQTLug-b4zwB",
        "mkGg9xJWZ2Bb",
        "xIOULjFJZ2Be",
        "CEcSbB_ZZ2BZ",
        "ycnCvcAVZ2Bi",
        "pMpQ-zrB11pp",
        "0WbM7XshZ2B2"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/SparkTutorial/blob/answer/(Spark_Tutorial)_%E4%B8%8A%E8%AA%B2%E8%AC%9B%E7%BE%A9_2019_Spark_DataFrame_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFD3Nh5xZ2A-",
        "outputId": "4f0b737b-2c8c-4843-f0fb-6e5585554da3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n",
        "bash init_env.sh"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-10 12:27:43--  https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/6bnwn8u2hz19s59/init_env.sh [following]\n",
            "--2020-11-10 12:27:43--  https://www.dropbox.com/s/raw/6bnwn8u2hz19s59/init_env.sh\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com/cd/0/inline/BC4T_YgMx95lEjcFDZGcvr7zAwnehWUCEHtxpEVfBLU9FYkrrK8Sf1OFqz3wi0QtR20t52FLSF0m9ccNvJ2DNPc09azQ1aC-vlYB2IYF1mtXvw/file# [following]\n",
            "--2020-11-10 12:27:44--  https://uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com/cd/0/inline/BC4T_YgMx95lEjcFDZGcvr7zAwnehWUCEHtxpEVfBLU9FYkrrK8Sf1OFqz3wi0QtR20t52FLSF0m9ccNvJ2DNPc09azQ1aC-vlYB2IYF1mtXvw/file\n",
            "Resolving uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com (uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com (uc032e1f48078e006cbc54fa0d4b.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 336 [text/plain]\n",
            "Saving to: ‘init_env.sh’\n",
            "\n",
            "init_env.sh         100%[===================>]     336  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-10 12:27:44 (37.9 MB/s) - ‘init_env.sh’ saved [336/336]\n",
            "\n",
            "--2020-11-10 12:27:44--  https://d3kbcqa49mib13.cloudfront.net/spark-2.2.0-bin-hadoop2.7.tgz\n",
            "Resolving d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)... 54.230.85.75, 54.230.85.222, 54.230.85.32, ...\n",
            "Connecting to d3kbcqa49mib13.cloudfront.net (d3kbcqa49mib13.cloudfront.net)|54.230.85.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 203728858 (194M) [application/x-tar]\n",
            "Saving to: ‘spark-2.2.0-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.2.0-bin-had 100%[===================>] 194.29M   181MB/s    in 1.1s    \n",
            "\n",
            "2020-11-10 12:27:45 (181 MB/s) - ‘spark-2.2.0-bin-hadoop2.7.tgz’ saved [203728858/203728858]\n",
            "\n",
            "spark-2.2.0-bin-hadoop2.7/\n",
            "spark-2.2.0-bin-hadoop2.7/NOTICE\n",
            "spark-2.2.0-bin-hadoop2.7/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-common-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-net-2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sketch_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/kryo-shaded-3.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-jvm-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/RoaringBitmap-0.5.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jets3t-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compress-1.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-1.7.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-format-2.3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jline-2.12.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-core-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/java-xmlbuilder-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xbean-asm5-shaded-4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-sql_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-jackson_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-ast_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-graphx_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-common_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-schema-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-recipes-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpclient-4.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-tags_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-databind-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-client-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-catalyst_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/paranamer-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/json4s-core_2.11-3.2.11.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-repl_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-xml_2.11-1.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/httpcore-4.4.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-yarn_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-hive_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-all-4.0.43.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/curator-framework-2.6.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-ipc-1.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mesos-1.0.0-shaded-protobuf.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-annotations-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-jackson-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-paranamer-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/janino-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr4-runtime-4.5.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/bcprov-jdk15on-1.51.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-launcher_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-graphite-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/lz4-1.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mesos_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mx4j-3.0.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-beanutils-core-1.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-encoding-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-mllib_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/mail-1.4.7.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/metrics-json-3.1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scalap-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/py4j-0.10.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/pmml-model-1.2.15.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/parquet-column-1.8.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/breeze_2.11-0.13.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill-java-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/avro-mapred-1.7.7-hadoop2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/snappy-java-1.1.2.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/base64-2.3.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-compiler-3.0.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/objenesis-2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/univocity-parsers-2.2.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.0.4.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/jackson-core-2.6.5.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/xz-1.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-reflect-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-compiler-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-core_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/spark-streaming_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/chill_2.11-0.8.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/scala-library-2.11.8.jar\n",
            "spark-2.2.0-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.2.0-bin-hadoop2.7/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.2.0-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.2.0-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.2.0-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.2.0-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/run-tests\n",
            "spark-2.2.0-bin-hadoop2.7/python/dist/\n",
            "spark-2.2.0-bin-hadoop2.7/python/setup.py\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.2.0-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.2.0-bin-hadoop2.7/python/README.md\n",
            "spark-2.2.0-bin-hadoop2.7/RELEASE\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.2.0-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.2.0-bin-hadoop2.7/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaFlumeEventCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumePollingEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/FlumeEventCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.2.0-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/scopt_2.11-3.3.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar\n",
            "spark-2.2.0-bin-hadoop2.7/data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/\n",
            "spark-2.2.0-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.2.0-bin-hadoop2.7/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/groupBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/covar_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sampleBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sql.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/year.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sign.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/orderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/otherwise.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/AFTSurvivalRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hashCode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.svmLinear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/minute.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createExternalTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/distinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.conf.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.jobj.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/md5.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cbrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/acos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/NaiveBayesModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sum.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isLocal.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableNames.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isStreaming.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toJSON.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFiles.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/except.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LDAModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/months_between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark_partition_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sumDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/awaitTermination.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/BisectingKMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/abs.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_format.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/withColumn.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sort_array.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/storageLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCurrentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ceil.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/floor.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sd.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structType.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.survreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/predict.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/count.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unhex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mean.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/instr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_unixtime.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/saveAsTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ltrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRHive.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.parquet.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/match.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/is.nan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.ml.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lag.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unpersist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/corr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJStatic.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LinearSVCModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gbt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/persist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_pop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/selectExpr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crc32.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.callJMethod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/with.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/generateAliasesForIntersectedCols.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GeneralizedLinearRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/IsotonicRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setLogLevel.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRightUnsigned.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/base64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/array_contains.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/expm1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.version.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/insertInto.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/SparkDataFrame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/merge.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dayofmonth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listDatabases.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summarize.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/from_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropDuplicates.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxCountDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkRSQL.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/LogisticRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stddev_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pivot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/showDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/between.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/struct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/subset.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/posexplode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftLeft.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hypot.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/recoverPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.stop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/translate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/drop.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GBTClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/shiftRight.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_replace.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/randomSplit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/length.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rowsBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.jdbc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/schema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toRadians.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/filter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bround.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createOrReplaceTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cancelJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/second.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/upper.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/head.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/limit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat_ws.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/when.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/FPGrowthModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/install.spark.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.newJObject.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempView.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unbase64.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/soundex.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.addFile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.bisectingKmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cosh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.mlp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ntile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kstest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dtypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/reverse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sinh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lda.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/createTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/negate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/asin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hash.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/toDegrees.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/columnfunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substring_index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.naiveBayes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/atan.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.isoreg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/factorial.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/countDistinct.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/quarter.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setCheckpointDir.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/least.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.text.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowOrderBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coalesce.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshByPath.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cume_dist.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dense_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/freqItems.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/getNumPartitions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KMeansModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.session.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/arrange.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/encode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.glm.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/isActive.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crossJoin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/uncacheTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/size.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/conv.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log10.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/collect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.stream.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/format_string.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/windowPartitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/union.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/stopQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/dropTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/endsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/startsWith.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nanvl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/mutate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explain.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cov.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var_samp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log2.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/registerTempTable-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lastProgress.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/attach.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/min.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ncol.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/month.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/window.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/partitionBy.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/percent_rank.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listFunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_utc_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/crosstab.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/take.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/exp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/column.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ifelse.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GaussianMixtureModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.logit.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/show.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/setJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/KSTest-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearJobGroup.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/last.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/printSchema.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rename.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rbind.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/over.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/MultilayerPerceptronClassificationModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/coltypes.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/datediff.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lead.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/summary.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/WindowSpec.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/unix_timestamp.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tanh.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listColumns.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/to_date.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.uiWebUrl.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/max.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/var.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/print.structField.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.als.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/alias.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sha1.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/status.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.orc.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/currentDatabase.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/write.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/round.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sparkR.init-deprecated.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/refreshTable.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nrow.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/pmod.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/intersect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rtrim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/substr.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/levenshtein.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/monotonically_increasing_id.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/checkpoint.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/decode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.lapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.json.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/RandomForestRegressionModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/trim.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/select.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/regexp_extract.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/as.data.frame.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rangeBetween.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/queryName.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sample.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lower.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/repartition.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/concat.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cast.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_add.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.fpGrowth.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/hour.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/initcap.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/explode.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/add_months.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/bitwiseNOT.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/approxQuantile.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/kurtosis.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/greatest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/first.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/read.df.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/rand.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/next_day.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/clearCache.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/nafunctions.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/row_number.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/lpad.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/skewness.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapplyCollect.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/locate.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/avg.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sqrt.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/cos.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/log1p.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/str.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/StreamingQuery.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ALSModel-class.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/ascii.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/listTables.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/sin.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/histogram.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/weekofyear.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/GroupedData.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/fitted.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/date_sub.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/tableToDF.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.getSparkFilesRootDirectory.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/join.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/gapply.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.randomForest.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.kmeans.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/html/spark.gaussianMixture.html\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.2.0-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scalacheck.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-Mockito.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-junit-interface.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-boto.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-SnapTree.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jpmml-model.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jbcrypt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-postgresql.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-DPark.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.2.0-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.2.0-bin-hadoop2.7/conf/\n",
            "spark-2.2.0-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.2.0-bin-hadoop2.7/LICENSE\n",
            "spark-2.2.0-bin-hadoop2.7/bin/\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.2.0-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.2.0-bin-hadoop2.7/bin/run-example\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.2.0-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.2.0-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/\n",
            "spark-2.2.0-bin-hadoop2.7/yarn/spark-2.2.0-yarn-shuffle.jar\n",
            "spark-2.2.0-bin-hadoop2.7/README.md\n",
            "環境初始化完畢\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u314rKd1Z2BC"
      },
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU1UwvjtaaSk"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc =SparkContext()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEFwbXSgauLA",
        "outputId": "8dd93588-dd7e-4a39-d9c9-bc4bb9f14442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-10 12:28:06--  https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv [following]\n",
            "--2020-11-10 12:28:07--  https://www.dropbox.com/s/raw/zkn3ba7pitv83el/pm2.5Taiwan.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com/cd/0/inline/BC6VnC3WB5g3pO2QhEu2RYgrLYmuzlDv3kqbOFLyfPEn5P7gfOi0gtXjxRYQJnKePNBK-I942BRrsnhHFN6kvkSKq6adnFsDI7DcUIWfcDY2yQ/file# [following]\n",
            "--2020-11-10 12:28:07--  https://uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com/cd/0/inline/BC6VnC3WB5g3pO2QhEu2RYgrLYmuzlDv3kqbOFLyfPEn5P7gfOi0gtXjxRYQJnKePNBK-I942BRrsnhHFN6kvkSKq6adnFsDI7DcUIWfcDY2yQ/file\n",
            "Resolving uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com (uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com (uc9c9c31fb4ed12ca9909505fc65.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50453822 (48M) [text/plain]\n",
            "Saving to: ‘pm25.csv’\n",
            "\n",
            "pm25.csv            100%[===================>]  48.12M  13.4MB/s    in 3.7s    \n",
            "\n",
            "2020-11-10 12:28:11 (13.0 MB/s) - ‘pm25.csv’ saved [50453822/50453822]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjbkqXx0ayTw"
      },
      "source": [
        "weather = sc.textFile(\"./pm25.csv\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9jzWplxZ2BE"
      },
      "source": [
        "weather_data_rdd = weather.map(lambda line : line.split(\",\"))\n",
        "# pm25schema = weather_data_rdd.first()\n",
        "# print(pm25schema)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FisGymJZ2BM"
      },
      "source": [
        "# 回想如何使用RDD計算求取2015年，大里每小時的平均pm25數值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPMxMS1f11qI"
      },
      "source": [
        "import math\n",
        "def remove_row_with_noise (x):\n",
        "    for i in range(3, len(x)):\n",
        "        if not x[i].isdecimal():\n",
        "            return False\n",
        "    return True \n",
        "\n",
        "def hourKeyGen(x):\n",
        "    hourkeypair = []\n",
        "    x=x[3:]\n",
        "    for i, value in enumerate(x):\n",
        "      print(i, value)\n",
        "      hourkeypair.append((i, float(value)))\n",
        "    return hourkeypair\n",
        "\n",
        "clean_weather_data = weather_data_rdd\\\n",
        "                    .filter(lambda x: x!=pm25schema)\\\n",
        "                    .filter(remove_row_with_noise)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KGoSn_c3viJ"
      },
      "source": [
        "dalipm25 = clean_weather_data.filter(lambda x: x[2]== \"PM2.5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMR32zDZ2BQ",
        "outputId": "6217c27a-36f4-4030-c117-d9d0c15789ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "count = dalipm25.count()\n",
        "HourSum = dalipm25\\\n",
        "            .flatMap(hourKeyGen)\\\n",
        "            .reduceByKey(lambda x,y: x+y)\\\n",
        "            .mapValues(lambda x: x/count)\\\n",
        "            .map(lambda x: (x[1],x[0])).top(24)\n",
        "\n",
        "print(HourSum)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(24.46175554690541, 10), (24.37315103152978, 11), (24.045299727520437, 9), (23.94949396652394, 12), (23.225282210977035, 13), (23.090064227325808, 8), (22.805809653561695, 20), (22.70732775398988, 21), (22.61857726741923, 19), (22.479028804982484, 14), (22.383952899961074, 22), (22.223968470221877, 18), (21.990803814713896, 15), (21.943022576878164, 23), (21.835879719735306, 17), (21.78376800311405, 7), (21.783476060724016, 16), (21.535227715064227, 0), (21.192244063838068, 1), (20.72007590502141, 2), (20.610694822888284, 6), (20.346876216426626, 3), (20.115025301673803, 5), (20.07342351109381, 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOldTm0PZ2BR"
      },
      "source": [
        "# 使用DataFrame 來計算每小時平均值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peojix3wauLF",
        "outputId": "807ff36e-5f8e-4e38-fdb2-6a3a27befb4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(dalipm25.first())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015/01/01', '龍潭', 'PM2.5', '46', '71', '76', '74', '65', '62', '56', '50', '52', '56', '54', '47', '40', '36', '37', '27', '30', '25', '26', '24', '18', '16', '11', '14']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLNcEsygc6oX"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TQFM4tixp6",
        "outputId": "1becc66c-afa8-4cb5-b0ef-8fc88ef34179",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i in dalipm25.take(5):\n",
        "  print(i)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2015/01/01', '龍潭', 'PM2.5', '46', '71', '76', '74', '65', '62', '56', '50', '52', '56', '54', '47', '40', '36', '37', '27', '30', '25', '26', '24', '18', '16', '11', '14']\n",
            "['2015/01/02', '龍潭', 'PM2.5', '15', '12', '9', '14', '17', '20', '18', '22', '21', '23', '18', '25', '24', '27', '18', '23', '18', '19', '18', '21', '23', '18', '19', '19']\n",
            "['2015/01/03', '龍潭', 'PM2.5', '14', '14', '9', '13', '12', '15', '9', '11', '14', '27', '27', '24', '16', '27', '38', '39', '35', '32', '38', '36', '34', '36', '36', '39']\n",
            "['2015/01/04', '龍潭', 'PM2.5', '42', '40', '39', '34', '33', '25', '24', '23', '31', '27', '34', '35', '35', '38', '46', '59', '53', '58', '61', '74', '80', '78', '70', '53']\n",
            "['2015/01/05', '龍潭', 'PM2.5', '42', '33', '24', '21', '20', '18', '16', '21', '21', '17', '13', '23', '36', '49', '61', '63', '68', '81', '91', '99', '76', '57', '34', '24']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pl9EZixkNiT",
        "outputId": "67018a38-3b83-447a-fc9d-1a6a2ff9cf36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "dalipm25row = dalipm25.map(lambda p:\n",
        "        Row(\n",
        "        date = p[0],\n",
        "        location = p[1],\n",
        "        measure = p[2],\n",
        "        hr_01 = float(p[3]), hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n",
        "        hr_06 = float(p[8]), hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n",
        "        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n",
        "        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n",
        "        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n",
        "    )\n",
        ")\n",
        "\n",
        "dalipm25row.take(5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(date='2015/01/01', hr_01=46.0, hr_02=71.0, hr_03=76.0, hr_04=74.0, hr_05=65.0, hr_06=62.0, hr_07=56.0, hr_08=50.0, hr_09=52.0, hr_10=56.0, hr_11=54.0, hr_12=47.0, hr_13=40.0, hr_14=36.0, hr_15=37.0, hr_16=27.0, hr_17=30.0, hr_18=25.0, hr_19=26.0, hr_20=24.0, hr_21=18.0, hr_22=16.0, hr_23=11.0, hr_24=14.0, location='龍潭', measure='PM2.5'),\n",
              " Row(date='2015/01/02', hr_01=15.0, hr_02=12.0, hr_03=9.0, hr_04=14.0, hr_05=17.0, hr_06=20.0, hr_07=18.0, hr_08=22.0, hr_09=21.0, hr_10=23.0, hr_11=18.0, hr_12=25.0, hr_13=24.0, hr_14=27.0, hr_15=18.0, hr_16=23.0, hr_17=18.0, hr_18=19.0, hr_19=18.0, hr_20=21.0, hr_21=23.0, hr_22=18.0, hr_23=19.0, hr_24=19.0, location='龍潭', measure='PM2.5'),\n",
              " Row(date='2015/01/03', hr_01=14.0, hr_02=14.0, hr_03=9.0, hr_04=13.0, hr_05=12.0, hr_06=15.0, hr_07=9.0, hr_08=11.0, hr_09=14.0, hr_10=27.0, hr_11=27.0, hr_12=24.0, hr_13=16.0, hr_14=27.0, hr_15=38.0, hr_16=39.0, hr_17=35.0, hr_18=32.0, hr_19=38.0, hr_20=36.0, hr_21=34.0, hr_22=36.0, hr_23=36.0, hr_24=39.0, location='龍潭', measure='PM2.5'),\n",
              " Row(date='2015/01/04', hr_01=42.0, hr_02=40.0, hr_03=39.0, hr_04=34.0, hr_05=33.0, hr_06=25.0, hr_07=24.0, hr_08=23.0, hr_09=31.0, hr_10=27.0, hr_11=34.0, hr_12=35.0, hr_13=35.0, hr_14=38.0, hr_15=46.0, hr_16=59.0, hr_17=53.0, hr_18=58.0, hr_19=61.0, hr_20=74.0, hr_21=80.0, hr_22=78.0, hr_23=70.0, hr_24=53.0, location='龍潭', measure='PM2.5'),\n",
              " Row(date='2015/01/05', hr_01=42.0, hr_02=33.0, hr_03=24.0, hr_04=21.0, hr_05=20.0, hr_06=18.0, hr_07=16.0, hr_08=21.0, hr_09=21.0, hr_10=17.0, hr_11=13.0, hr_12=23.0, hr_13=36.0, hr_14=49.0, hr_15=61.0, hr_16=63.0, hr_17=68.0, hr_18=81.0, hr_19=91.0, hr_20=99.0, hr_21=76.0, hr_22=57.0, hr_23=34.0, hr_24=24.0, location='龍潭', measure='PM2.5')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF182dzHZ2BU",
        "outputId": "4f000fae-d180-42cb-a9e8-610f55952166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 46.0| 71.0| 76.0| 74.0| 65.0| 62.0| 56.0| 50.0| 52.0| 56.0| 54.0| 47.0| 40.0| 36.0| 37.0| 27.0| 30.0| 25.0| 26.0| 24.0| 18.0| 16.0| 11.0| 14.0|      龍潭|  PM2.5|\n",
            "|2015/01/02| 15.0| 12.0|  9.0| 14.0| 17.0| 20.0| 18.0| 22.0| 21.0| 23.0| 18.0| 25.0| 24.0| 27.0| 18.0| 23.0| 18.0| 19.0| 18.0| 21.0| 23.0| 18.0| 19.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/03| 14.0| 14.0|  9.0| 13.0| 12.0| 15.0|  9.0| 11.0| 14.0| 27.0| 27.0| 24.0| 16.0| 27.0| 38.0| 39.0| 35.0| 32.0| 38.0| 36.0| 34.0| 36.0| 36.0| 39.0|      龍潭|  PM2.5|\n",
            "|2015/01/04| 42.0| 40.0| 39.0| 34.0| 33.0| 25.0| 24.0| 23.0| 31.0| 27.0| 34.0| 35.0| 35.0| 38.0| 46.0| 59.0| 53.0| 58.0| 61.0| 74.0| 80.0| 78.0| 70.0| 53.0|      龍潭|  PM2.5|\n",
            "|2015/01/05| 42.0| 33.0| 24.0| 21.0| 20.0| 18.0| 16.0| 21.0| 21.0| 17.0| 13.0| 23.0| 36.0| 49.0| 61.0| 63.0| 68.0| 81.0| 91.0| 99.0| 76.0| 57.0| 34.0| 24.0|      龍潭|  PM2.5|\n",
            "|2015/01/06| 19.0| 13.0| 11.0| 16.0| 11.0|  9.0|  6.0|  8.0|  8.0|  7.0| 11.0| 19.0| 33.0| 32.0| 34.0| 30.0| 32.0| 39.0| 53.0| 73.0| 68.0| 61.0| 36.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/08| 22.0| 30.0| 28.0| 37.0| 34.0| 40.0| 34.0| 41.0| 41.0| 50.0| 54.0| 51.0| 48.0| 43.0| 27.0| 26.0| 39.0| 45.0| 40.0| 42.0| 39.0| 32.0| 24.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/09| 23.0| 23.0| 17.0| 15.0| 13.0| 14.0| 18.0| 26.0| 27.0| 30.0| 25.0| 23.0| 18.0| 17.0| 26.0| 26.0| 36.0| 29.0| 36.0| 26.0| 30.0| 29.0| 27.0| 22.0|      龍潭|  PM2.5|\n",
            "|2015/01/10| 20.0| 18.0| 13.0|  9.0| 13.0| 11.0| 11.0| 10.0| 13.0| 17.0| 15.0| 20.0| 10.0| 18.0| 14.0| 17.0| 14.0| 13.0| 16.0| 20.0| 27.0| 31.0| 27.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/11| 31.0| 30.0| 32.0| 30.0| 33.0| 33.0| 32.0| 35.0| 26.0| 25.0| 18.0| 25.0| 22.0| 26.0| 25.0| 29.0| 24.0| 25.0| 24.0| 33.0| 38.0| 43.0| 41.0| 33.0|      龍潭|  PM2.5|\n",
            "|2015/01/13| 11.0| 12.0| 11.0| 13.0| 16.0| 11.0| 14.0|  8.0| 10.0| 10.0| 22.0| 25.0| 31.0| 31.0| 43.0| 36.0| 43.0| 34.0| 37.0| 25.0| 20.0| 14.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/14|  3.0|  5.0|  5.0|  5.0|  3.0|  4.0|  5.0|  7.0|  9.0| 11.0| 17.0| 10.0| 13.0|  6.0|  6.0| 12.0| 16.0| 23.0| 16.0| 15.0| 12.0| 12.0| 14.0| 12.0|      龍潭|  PM2.5|\n",
            "|2015/01/15| 14.0|  8.0| 11.0| 13.0| 20.0| 23.0| 23.0| 21.0| 17.0| 19.0| 20.0| 23.0| 20.0| 24.0| 23.0| 18.0| 13.0| 13.0| 14.0| 19.0| 14.0| 16.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/16|  8.0| 14.0| 15.0| 10.0|  8.0|  6.0| 11.0| 11.0| 21.0| 20.0| 25.0| 24.0| 22.0| 24.0| 19.0| 31.0| 18.0| 32.0| 31.0| 47.0| 38.0| 36.0| 25.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/17| 23.0| 33.0| 31.0| 30.0| 35.0| 37.0| 50.0| 57.0| 78.0| 77.0| 72.0| 59.0| 63.0| 61.0| 67.0| 61.0| 64.0| 65.0| 71.0| 63.0| 69.0| 75.0| 79.0| 73.0|      龍潭|  PM2.5|\n",
            "|2015/01/18| 55.0| 51.0| 40.0| 39.0| 32.0| 31.0| 28.0| 27.0| 27.0| 31.0| 30.0| 20.0| 17.0| 17.0| 23.0| 15.0| 14.0|  8.0| 15.0| 24.0| 22.0| 22.0| 13.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/19| 12.0| 11.0| 11.0| 18.0| 22.0| 22.0| 22.0| 25.0| 26.0| 39.0| 51.0| 51.0| 43.0| 42.0| 46.0| 41.0| 34.0| 37.0| 40.0| 39.0| 37.0| 36.0| 32.0| 34.0|      龍潭|  PM2.5|\n",
            "|2015/01/20| 34.0| 41.0| 35.0| 34.0| 36.0| 38.0| 38.0| 36.0| 38.0| 39.0| 38.0| 38.0| 36.0| 38.0| 38.0| 37.0| 38.0| 33.0| 37.0| 35.0| 46.0| 32.0| 28.0| 29.0|      龍潭|  PM2.5|\n",
            "|2015/01/21| 34.0| 39.0| 33.0| 43.0| 46.0| 49.0| 40.0| 28.0| 27.0| 21.0| 27.0| 24.0| 29.0| 35.0| 32.0| 38.0| 37.0| 45.0| 42.0| 40.0| 41.0| 43.0| 47.0| 40.0|      龍潭|  PM2.5|\n",
            "|2015/01/22| 36.0| 34.0| 30.0| 33.0| 27.0| 28.0| 23.0| 26.0| 26.0| 27.0| 28.0| 38.0| 49.0| 58.0| 68.0| 83.0| 87.0| 83.0| 71.0| 70.0| 76.0| 73.0| 70.0| 59.0|      龍潭|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRw_d4rMJizR",
        "outputId": "aa493d72-c3c6-4ab6-9518-1e9bc7ea1795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.filter(df[\"date\"]==\"2015/01/22\").select(\"hr_01\", \"hr_02\", \"location\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+--------+\n",
            "|hr_01|hr_02|location|\n",
            "+-----+-----+--------+\n",
            "| 36.0| 34.0|      龍潭|\n",
            "| 48.0| 43.0|      麥寮|\n",
            "| 41.0| 40.0|      馬祖|\n",
            "| 36.0| 35.0|      馬公|\n",
            "| 39.0| 34.0|      頭份|\n",
            "| 26.0| 33.0|      陽明|\n",
            "| 26.0| 27.0|      關山|\n",
            "| 71.0| 74.0|      金門|\n",
            "| 37.0| 37.0|      觀音|\n",
            "| 21.0| 23.0|      萬里|\n",
            "| 30.0| 25.0|      萬華|\n",
            "| 42.0| 38.0|      苗栗|\n",
            "| 21.0| 23.0|      花蓮|\n",
            "| 39.0| 39.0|      臺西|\n",
            "| 26.0| 27.0|      臺東|\n",
            "| 53.0| 52.0|      臺南|\n",
            "| 45.0| 45.0|      美濃|\n",
            "| 48.0| 44.0|      線西|\n",
            "| 28.0| 18.0|      竹東|\n",
            "| 56.0| 57.0|      潮州|\n",
            "+-----+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1VGONJlxgj",
        "outputId": "a4cc8b63-eda7-42e9-c491-fa9b06280c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.filter(df[\"date\"]==\"2015/01/22\").select(\"hr_01\",\"location\").show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------+\n",
            "|hr_01|location|\n",
            "+-----+--------+\n",
            "| 36.0|      龍潭|\n",
            "| 48.0|      麥寮|\n",
            "| 41.0|      馬祖|\n",
            "| 36.0|      馬公|\n",
            "| 39.0|      頭份|\n",
            "| 26.0|      陽明|\n",
            "| 26.0|      關山|\n",
            "| 71.0|      金門|\n",
            "| 37.0|      觀音|\n",
            "| 21.0|      萬里|\n",
            "| 30.0|      萬華|\n",
            "| 42.0|      苗栗|\n",
            "| 21.0|      花蓮|\n",
            "| 39.0|      臺西|\n",
            "| 26.0|      臺東|\n",
            "| 53.0|      臺南|\n",
            "| 45.0|      美濃|\n",
            "| 48.0|      線西|\n",
            "| 28.0|      竹東|\n",
            "| 56.0|      潮州|\n",
            "+-----+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H3TgwxKKAqc",
        "outputId": "440451c9-6b89-421b-90fc-2b1f41e5add6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "|summary|      date|             hr_01|             hr_02|             hr_03|             hr_04|            hr_05|             hr_06|             hr_07|            hr_08|             hr_09|             hr_10|            hr_11|            hr_12|             hr_13|             hr_14|             hr_15|             hr_16|             hr_17|             hr_18|             hr_19|            hr_20|             hr_21|            hr_22|             hr_23|             hr_24|location|measure|\n",
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "|  count|     20552|             20552|             20552|             20552|             20552|            20552|             20552|             20552|            20552|             20552|             20552|            20552|            20552|             20552|             20552|             20552|             20552|             20552|             20552|             20552|            20552|             20552|            20552|             20552|             20552|   20552|  20552|\n",
            "|   mean|      null|21.535227715064227|21.192244063838068| 20.72007590502141|20.346876216426626|20.07342351109381|20.115025301673803|20.610694822888284|21.78376800311405|23.090064227325808|24.045299727520437|24.46175554690541|24.37315103152978| 23.94949396652394|23.225282210977035|22.479028804982484|21.990803814713896|21.783476060724016|21.835879719735306|22.223968470221877|22.61857726741923|22.805809653561695|22.70732775398988|22.383952899961074|21.943022576878164|    null|   null|\n",
            "| stddev|      null|18.047747935517563|18.002083373211494|17.684292537552203|  17.4534702882011|17.18736705530248|17.056695425787385| 16.89439468324324|17.16904685103528| 17.66291882260712| 18.17352764324633|18.24608616526683|17.87181704312976|17.469348055660767|16.964951897491453|16.557825826425404|16.256845051911093|16.189240332048954| 16.24085092555725|   16.686725097183|17.21240253400489|17.669545429722465|18.05949134932172| 18.25233788955407|18.114875567427784|    null|   null|\n",
            "|    min|2015/01/01|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|              0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|              0.0|               0.0|               0.0|      三義|  PM2.5|\n",
            "|    max|2015/12/31|             296.0|             301.0|             148.0|             166.0|            189.0|             189.0|             175.0|            165.0|             175.0|             189.0|            213.0|            238.0|             177.0|             145.0|             140.0|             136.0|             142.0|             152.0|             170.0|            176.0|             181.0|            319.0|             238.0|             145.0|      龍潭|  PM2.5|\n",
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTLug-b4zwB"
      },
      "source": [
        "# 直接透過rdd生成data frame. toDF()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9la--euNJXT"
      },
      "source": [
        "pm25schema = weather_data_rdd.first()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVbTyRvDNVxl"
      },
      "source": [
        "pm25schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpycWwEed5PR"
      },
      "source": [
        "dalipm25.toDF().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpezXkfDfPNX"
      },
      "source": [
        "pm25schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqp0-TzbAhte"
      },
      "source": [
        "df2 = dalipm25.toDF(pm25schema)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bku4vR-hfcT",
        "outputId": "53ffaa4d-8e6f-4be4-e766-27a2a1e77dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df2.schema"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType(List(StructField(日期,StringType,true),StructField(測站,StringType,true),StructField(測項,StringType,true),StructField(00,StringType,true),StructField(01,StringType,true),StructField(02,StringType,true),StructField(03,StringType,true),StructField(04,StringType,true),StructField(05,StringType,true),StructField(06,StringType,true),StructField(07,StringType,true),StructField(08,StringType,true),StructField(09,StringType,true),StructField(10,StringType,true),StructField(11,StringType,true),StructField(12,StringType,true),StructField(13,StringType,true),StructField(14,StringType,true),StructField(15,StringType,true),StructField(16,StringType,true),StructField(17,StringType,true),StructField(18,StringType,true),StructField(19,StringType,true),StructField(20,StringType,true),StructField(21,StringType,true),StructField(22,StringType,true),StructField(23,StringType,true)))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByodqKk3knMK"
      },
      "source": [
        "dfpm25 = dalipm25.toDF(pm25schema)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuzt57KyBYL5",
        "outputId": "e3205498-72f9-4ccd-e641-9c9968b66d16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfpm25.columns"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['日期',\n",
              " '測站',\n",
              " '測項',\n",
              " '00',\n",
              " '01',\n",
              " '02',\n",
              " '03',\n",
              " '04',\n",
              " '05',\n",
              " '06',\n",
              " '07',\n",
              " '08',\n",
              " '09',\n",
              " '10',\n",
              " '11',\n",
              " '12',\n",
              " '13',\n",
              " '14',\n",
              " '15',\n",
              " '16',\n",
              " '17',\n",
              " '18',\n",
              " '19',\n",
              " '20',\n",
              " '21',\n",
              " '22',\n",
              " '23']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4IHhqFyJ8Z"
      },
      "source": [
        "for i in dfpm25.columns[3:]:\n",
        "  dfpm25 = dfpm25.withColumn(i, dfpm25[i].cast(\"double\"))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYRCksyPll5S",
        "outputId": "59aff2cb-3820-4f14-d3bf-5cd3ff3602ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dfpm25.printSchema()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- 日期: string (nullable = true)\n",
            " |-- 測站: string (nullable = true)\n",
            " |-- 測項: string (nullable = true)\n",
            " |-- 00: double (nullable = true)\n",
            " |-- 01: double (nullable = true)\n",
            " |-- 02: double (nullable = true)\n",
            " |-- 03: double (nullable = true)\n",
            " |-- 04: double (nullable = true)\n",
            " |-- 05: double (nullable = true)\n",
            " |-- 06: double (nullable = true)\n",
            " |-- 07: double (nullable = true)\n",
            " |-- 08: double (nullable = true)\n",
            " |-- 09: double (nullable = true)\n",
            " |-- 10: double (nullable = true)\n",
            " |-- 11: double (nullable = true)\n",
            " |-- 12: double (nullable = true)\n",
            " |-- 13: double (nullable = true)\n",
            " |-- 14: double (nullable = true)\n",
            " |-- 15: double (nullable = true)\n",
            " |-- 16: double (nullable = true)\n",
            " |-- 17: double (nullable = true)\n",
            " |-- 18: double (nullable = true)\n",
            " |-- 19: double (nullable = true)\n",
            " |-- 20: double (nullable = true)\n",
            " |-- 21: double (nullable = true)\n",
            " |-- 22: double (nullable = true)\n",
            " |-- 23: double (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkGg9xJWZ2Bb"
      },
      "source": [
        "# 使用 DataFrame.filter() 來進行row資料之條件計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtr4GV5bZ2Ba",
        "outputId": "560802a7-9d2c-4ed5-fdc7-8ae41d1cea6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.filter(df.hr_01>30).show()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 46.0| 71.0| 76.0| 74.0| 65.0| 62.0| 56.0| 50.0| 52.0| 56.0| 54.0| 47.0| 40.0| 36.0| 37.0| 27.0| 30.0| 25.0| 26.0| 24.0| 18.0| 16.0| 11.0| 14.0|      龍潭|  PM2.5|\n",
            "|2015/01/04| 42.0| 40.0| 39.0| 34.0| 33.0| 25.0| 24.0| 23.0| 31.0| 27.0| 34.0| 35.0| 35.0| 38.0| 46.0| 59.0| 53.0| 58.0| 61.0| 74.0| 80.0| 78.0| 70.0| 53.0|      龍潭|  PM2.5|\n",
            "|2015/01/05| 42.0| 33.0| 24.0| 21.0| 20.0| 18.0| 16.0| 21.0| 21.0| 17.0| 13.0| 23.0| 36.0| 49.0| 61.0| 63.0| 68.0| 81.0| 91.0| 99.0| 76.0| 57.0| 34.0| 24.0|      龍潭|  PM2.5|\n",
            "|2015/01/11| 31.0| 30.0| 32.0| 30.0| 33.0| 33.0| 32.0| 35.0| 26.0| 25.0| 18.0| 25.0| 22.0| 26.0| 25.0| 29.0| 24.0| 25.0| 24.0| 33.0| 38.0| 43.0| 41.0| 33.0|      龍潭|  PM2.5|\n",
            "|2015/01/18| 55.0| 51.0| 40.0| 39.0| 32.0| 31.0| 28.0| 27.0| 27.0| 31.0| 30.0| 20.0| 17.0| 17.0| 23.0| 15.0| 14.0|  8.0| 15.0| 24.0| 22.0| 22.0| 13.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/20| 34.0| 41.0| 35.0| 34.0| 36.0| 38.0| 38.0| 36.0| 38.0| 39.0| 38.0| 38.0| 36.0| 38.0| 38.0| 37.0| 38.0| 33.0| 37.0| 35.0| 46.0| 32.0| 28.0| 29.0|      龍潭|  PM2.5|\n",
            "|2015/01/21| 34.0| 39.0| 33.0| 43.0| 46.0| 49.0| 40.0| 28.0| 27.0| 21.0| 27.0| 24.0| 29.0| 35.0| 32.0| 38.0| 37.0| 45.0| 42.0| 40.0| 41.0| 43.0| 47.0| 40.0|      龍潭|  PM2.5|\n",
            "|2015/01/22| 36.0| 34.0| 30.0| 33.0| 27.0| 28.0| 23.0| 26.0| 26.0| 27.0| 28.0| 38.0| 49.0| 58.0| 68.0| 83.0| 87.0| 83.0| 71.0| 70.0| 76.0| 73.0| 70.0| 59.0|      龍潭|  PM2.5|\n",
            "|2015/01/23| 53.0| 49.0| 47.0| 41.0| 41.0| 46.0| 42.0| 48.0| 41.0| 51.0| 40.0| 47.0| 48.0| 43.0| 42.0| 38.0| 58.0| 48.0| 53.0| 43.0| 53.0| 52.0| 48.0| 47.0|      龍潭|  PM2.5|\n",
            "|2015/01/24| 38.0| 25.0| 15.0| 14.0| 16.0| 18.0|  9.0| 10.0|  4.0| 15.0| 20.0| 28.0| 38.0| 41.0| 50.0| 38.0| 40.0| 41.0| 51.0| 62.0| 64.0| 65.0| 49.0| 40.0|      龍潭|  PM2.5|\n",
            "|2015/01/25| 34.0| 41.0| 33.0| 29.0| 15.0| 26.0| 27.0| 42.0| 43.0| 54.0| 57.0| 59.0| 52.0| 43.0| 47.0| 49.0| 54.0| 53.0| 52.0| 57.0| 55.0| 48.0| 34.0| 22.0|      龍潭|  PM2.5|\n",
            "|2015/02/05| 38.0| 29.0| 32.0| 32.0| 31.0| 31.0| 41.0| 60.0| 74.0| 90.0| 97.0| 98.0| 99.0|100.0| 99.0| 94.0| 92.0| 91.0| 86.0| 85.0| 76.0| 66.0| 59.0| 55.0|      龍潭|  PM2.5|\n",
            "|2015/02/06| 49.0| 34.0| 27.0| 22.0| 27.0| 21.0| 17.0| 11.0| 12.0| 16.0|  9.0| 13.0| 13.0| 13.0| 13.0| 14.0| 17.0| 19.0| 20.0| 26.0| 23.0| 18.0| 12.0| 10.0|      龍潭|  PM2.5|\n",
            "|2015/02/12| 36.0| 29.0| 35.0| 31.0| 43.0| 41.0| 51.0| 52.0| 58.0| 58.0| 58.0| 52.0| 41.0| 35.0| 30.0| 32.0| 39.0| 43.0| 53.0| 47.0| 50.0| 42.0| 34.0| 30.0|      龍潭|  PM2.5|\n",
            "|2015/02/14| 36.0| 37.0| 33.0| 33.0| 31.0| 29.0| 31.0| 31.0| 31.0| 36.0| 41.0| 49.0| 57.0| 57.0| 56.0| 43.0| 48.0| 49.0| 68.0| 70.0| 90.0| 87.0| 75.0| 59.0|      龍潭|  PM2.5|\n",
            "|2015/02/15| 43.0| 48.0| 34.0| 37.0| 32.0| 38.0| 29.0| 33.0| 27.0| 31.0| 35.0| 52.0| 61.0| 60.0| 52.0| 53.0| 60.0| 63.0| 63.0| 55.0| 51.0| 48.0| 40.0| 38.0|      龍潭|  PM2.5|\n",
            "|2015/02/18| 49.0| 56.0| 45.0| 39.0| 42.0| 42.0| 48.0| 46.0| 48.0| 43.0| 45.0| 45.0| 49.0| 38.0| 42.0| 38.0| 45.0| 42.0| 37.0| 41.0| 38.0| 38.0| 29.0| 26.0|      龍潭|  PM2.5|\n",
            "|2015/02/19| 31.0| 29.0| 31.0| 29.0| 33.0| 31.0| 39.0| 39.0| 42.0| 39.0| 40.0| 37.0| 32.0| 32.0| 32.0| 27.0| 24.0| 20.0| 26.0| 20.0| 28.0| 23.0| 27.0| 23.0|      龍潭|  PM2.5|\n",
            "|2015/02/22| 68.0| 70.0| 48.0| 39.0| 26.0| 36.0| 49.0| 52.0| 38.0| 23.0| 25.0| 35.0| 36.0| 34.0| 32.0| 32.0| 28.0| 27.0| 24.0| 26.0| 25.0| 32.0| 27.0| 22.0|      龍潭|  PM2.5|\n",
            "|2015/03/15| 46.0| 49.0| 42.0| 39.0| 37.0| 29.0| 27.0| 23.0| 26.0| 29.0| 34.0| 31.0| 30.0| 29.0| 39.0| 48.0| 50.0| 56.0| 49.0| 56.0| 65.0| 73.0| 71.0| 52.0|      龍潭|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOULjFJZ2Be"
      },
      "source": [
        "# 使用 DataFrame.select() 來進行資料之Projection http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCXgch-_Z2Bc",
        "outputId": "b64421d9-b65a-434d-a00e-a4ec7354d311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.select(\"hr_01\", \"hr_02\", \"location\", \"measure\").show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+-----+--------+-------+\n",
            "|hr_01|hr_02|location|measure|\n",
            "+-----+-----+--------+-------+\n",
            "| 46.0| 71.0|      龍潭|  PM2.5|\n",
            "| 15.0| 12.0|      龍潭|  PM2.5|\n",
            "| 14.0| 14.0|      龍潭|  PM2.5|\n",
            "| 42.0| 40.0|      龍潭|  PM2.5|\n",
            "| 42.0| 33.0|      龍潭|  PM2.5|\n",
            "| 19.0| 13.0|      龍潭|  PM2.5|\n",
            "| 22.0| 30.0|      龍潭|  PM2.5|\n",
            "| 23.0| 23.0|      龍潭|  PM2.5|\n",
            "| 20.0| 18.0|      龍潭|  PM2.5|\n",
            "| 31.0| 30.0|      龍潭|  PM2.5|\n",
            "| 11.0| 12.0|      龍潭|  PM2.5|\n",
            "|  3.0|  5.0|      龍潭|  PM2.5|\n",
            "| 14.0|  8.0|      龍潭|  PM2.5|\n",
            "|  8.0| 14.0|      龍潭|  PM2.5|\n",
            "| 23.0| 33.0|      龍潭|  PM2.5|\n",
            "| 55.0| 51.0|      龍潭|  PM2.5|\n",
            "| 12.0| 11.0|      龍潭|  PM2.5|\n",
            "| 34.0| 41.0|      龍潭|  PM2.5|\n",
            "| 34.0| 39.0|      龍潭|  PM2.5|\n",
            "| 36.0| 34.0|      龍潭|  PM2.5|\n",
            "+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6sInWJZ2Bg"
      },
      "source": [
        "## 使用 DataFrame.describe() 來進行DataFrame or Column 資料之統計 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-vy-Ms1Z2Bh",
        "outputId": "9b1e03e5-f9ad-40f4-cd83-3c6d04fb504a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "|summary|      date|             hr_01|             hr_02|             hr_03|             hr_04|            hr_05|             hr_06|             hr_07|            hr_08|             hr_09|             hr_10|            hr_11|            hr_12|             hr_13|             hr_14|             hr_15|             hr_16|             hr_17|             hr_18|             hr_19|            hr_20|             hr_21|            hr_22|             hr_23|             hr_24|location|measure|\n",
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "|  count|     20552|             20552|             20552|             20552|             20552|            20552|             20552|             20552|            20552|             20552|             20552|            20552|            20552|             20552|             20552|             20552|             20552|             20552|             20552|             20552|            20552|             20552|            20552|             20552|             20552|   20552|  20552|\n",
            "|   mean|      null|21.535227715064227|21.192244063838068| 20.72007590502141|20.346876216426626|20.07342351109381|20.115025301673803|20.610694822888284|21.78376800311405|23.090064227325808|24.045299727520437|24.46175554690541|24.37315103152978| 23.94949396652394|23.225282210977035|22.479028804982484|21.990803814713896|21.783476060724016|21.835879719735306|22.223968470221877|22.61857726741923|22.805809653561695|22.70732775398988|22.383952899961074|21.943022576878164|    null|   null|\n",
            "| stddev|      null|18.047747935517563|18.002083373211494|17.684292537552203|  17.4534702882011|17.18736705530248|17.056695425787385| 16.89439468324324|17.16904685103528| 17.66291882260712| 18.17352764324633|18.24608616526683|17.87181704312976|17.469348055660767|16.964951897491453|16.557825826425404|16.256845051911093|16.189240332048954| 16.24085092555725|   16.686725097183|17.21240253400489|17.669545429722465|18.05949134932172| 18.25233788955407|18.114875567427784|    null|   null|\n",
            "|    min|2015/01/01|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|               0.0|               0.0|              0.0|              0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|               0.0|              0.0|               0.0|              0.0|               0.0|               0.0|      三義|  PM2.5|\n",
            "|    max|2015/12/31|             296.0|             301.0|             148.0|             166.0|            189.0|             189.0|             175.0|            165.0|             175.0|             189.0|            213.0|            238.0|             177.0|             145.0|             140.0|             136.0|             142.0|             152.0|             170.0|            176.0|             181.0|            319.0|             238.0|             145.0|      龍潭|  PM2.5|\n",
            "+-------+----------+------------------+------------------+------------------+------------------+-----------------+------------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+------------------+-----------------+------------------+------------------+--------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhbMcF9VkG66"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcSbB_ZZ2BZ"
      },
      "source": [
        "# 使用 DataFrame.agg() 來進行column數值之統計計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP5lLypj1P8T"
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-PTxlE0Z2Be",
        "outputId": "5ce15c1b-79c0-4974-96d8-18e14d45822e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "df.agg(F.mean(df.hr_01),F.mean(df.hr_02)).show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------+------------------+\n",
            "|        avg(hr_01)|        avg(hr_02)|\n",
            "+------------------+------------------+\n",
            "|21.535227715064227|21.192244063838068|\n",
            "+------------------+------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofza07KUcAJ3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jtfh2MQHTY"
      },
      "source": [
        "#使用 DataFrame.withColumn() 來進行column新增column http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaksZR0sPec",
        "outputId": "9ae3292a-aa4d-4a96-c93a-f9e9072080f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 46.0| 71.0| 76.0| 74.0| 65.0| 62.0| 56.0| 50.0| 52.0| 56.0| 54.0| 47.0| 40.0| 36.0| 37.0| 27.0| 30.0| 25.0| 26.0| 24.0| 18.0| 16.0| 11.0| 14.0|      龍潭|  PM2.5|\n",
            "|2015/01/02| 15.0| 12.0|  9.0| 14.0| 17.0| 20.0| 18.0| 22.0| 21.0| 23.0| 18.0| 25.0| 24.0| 27.0| 18.0| 23.0| 18.0| 19.0| 18.0| 21.0| 23.0| 18.0| 19.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/03| 14.0| 14.0|  9.0| 13.0| 12.0| 15.0|  9.0| 11.0| 14.0| 27.0| 27.0| 24.0| 16.0| 27.0| 38.0| 39.0| 35.0| 32.0| 38.0| 36.0| 34.0| 36.0| 36.0| 39.0|      龍潭|  PM2.5|\n",
            "|2015/01/04| 42.0| 40.0| 39.0| 34.0| 33.0| 25.0| 24.0| 23.0| 31.0| 27.0| 34.0| 35.0| 35.0| 38.0| 46.0| 59.0| 53.0| 58.0| 61.0| 74.0| 80.0| 78.0| 70.0| 53.0|      龍潭|  PM2.5|\n",
            "|2015/01/05| 42.0| 33.0| 24.0| 21.0| 20.0| 18.0| 16.0| 21.0| 21.0| 17.0| 13.0| 23.0| 36.0| 49.0| 61.0| 63.0| 68.0| 81.0| 91.0| 99.0| 76.0| 57.0| 34.0| 24.0|      龍潭|  PM2.5|\n",
            "|2015/01/06| 19.0| 13.0| 11.0| 16.0| 11.0|  9.0|  6.0|  8.0|  8.0|  7.0| 11.0| 19.0| 33.0| 32.0| 34.0| 30.0| 32.0| 39.0| 53.0| 73.0| 68.0| 61.0| 36.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/08| 22.0| 30.0| 28.0| 37.0| 34.0| 40.0| 34.0| 41.0| 41.0| 50.0| 54.0| 51.0| 48.0| 43.0| 27.0| 26.0| 39.0| 45.0| 40.0| 42.0| 39.0| 32.0| 24.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/09| 23.0| 23.0| 17.0| 15.0| 13.0| 14.0| 18.0| 26.0| 27.0| 30.0| 25.0| 23.0| 18.0| 17.0| 26.0| 26.0| 36.0| 29.0| 36.0| 26.0| 30.0| 29.0| 27.0| 22.0|      龍潭|  PM2.5|\n",
            "|2015/01/10| 20.0| 18.0| 13.0|  9.0| 13.0| 11.0| 11.0| 10.0| 13.0| 17.0| 15.0| 20.0| 10.0| 18.0| 14.0| 17.0| 14.0| 13.0| 16.0| 20.0| 27.0| 31.0| 27.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/11| 31.0| 30.0| 32.0| 30.0| 33.0| 33.0| 32.0| 35.0| 26.0| 25.0| 18.0| 25.0| 22.0| 26.0| 25.0| 29.0| 24.0| 25.0| 24.0| 33.0| 38.0| 43.0| 41.0| 33.0|      龍潭|  PM2.5|\n",
            "|2015/01/13| 11.0| 12.0| 11.0| 13.0| 16.0| 11.0| 14.0|  8.0| 10.0| 10.0| 22.0| 25.0| 31.0| 31.0| 43.0| 36.0| 43.0| 34.0| 37.0| 25.0| 20.0| 14.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/14|  3.0|  5.0|  5.0|  5.0|  3.0|  4.0|  5.0|  7.0|  9.0| 11.0| 17.0| 10.0| 13.0|  6.0|  6.0| 12.0| 16.0| 23.0| 16.0| 15.0| 12.0| 12.0| 14.0| 12.0|      龍潭|  PM2.5|\n",
            "|2015/01/15| 14.0|  8.0| 11.0| 13.0| 20.0| 23.0| 23.0| 21.0| 17.0| 19.0| 20.0| 23.0| 20.0| 24.0| 23.0| 18.0| 13.0| 13.0| 14.0| 19.0| 14.0| 16.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/16|  8.0| 14.0| 15.0| 10.0|  8.0|  6.0| 11.0| 11.0| 21.0| 20.0| 25.0| 24.0| 22.0| 24.0| 19.0| 31.0| 18.0| 32.0| 31.0| 47.0| 38.0| 36.0| 25.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/17| 23.0| 33.0| 31.0| 30.0| 35.0| 37.0| 50.0| 57.0| 78.0| 77.0| 72.0| 59.0| 63.0| 61.0| 67.0| 61.0| 64.0| 65.0| 71.0| 63.0| 69.0| 75.0| 79.0| 73.0|      龍潭|  PM2.5|\n",
            "|2015/01/18| 55.0| 51.0| 40.0| 39.0| 32.0| 31.0| 28.0| 27.0| 27.0| 31.0| 30.0| 20.0| 17.0| 17.0| 23.0| 15.0| 14.0|  8.0| 15.0| 24.0| 22.0| 22.0| 13.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/19| 12.0| 11.0| 11.0| 18.0| 22.0| 22.0| 22.0| 25.0| 26.0| 39.0| 51.0| 51.0| 43.0| 42.0| 46.0| 41.0| 34.0| 37.0| 40.0| 39.0| 37.0| 36.0| 32.0| 34.0|      龍潭|  PM2.5|\n",
            "|2015/01/20| 34.0| 41.0| 35.0| 34.0| 36.0| 38.0| 38.0| 36.0| 38.0| 39.0| 38.0| 38.0| 36.0| 38.0| 38.0| 37.0| 38.0| 33.0| 37.0| 35.0| 46.0| 32.0| 28.0| 29.0|      龍潭|  PM2.5|\n",
            "|2015/01/21| 34.0| 39.0| 33.0| 43.0| 46.0| 49.0| 40.0| 28.0| 27.0| 21.0| 27.0| 24.0| 29.0| 35.0| 32.0| 38.0| 37.0| 45.0| 42.0| 40.0| 41.0| 43.0| 47.0| 40.0|      龍潭|  PM2.5|\n",
            "|2015/01/22| 36.0| 34.0| 30.0| 33.0| 27.0| 28.0| 23.0| 26.0| 26.0| 27.0| 28.0| 38.0| 49.0| 58.0| 68.0| 83.0| 87.0| 83.0| 71.0| 70.0| 76.0| 73.0| 70.0| 59.0|      龍潭|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz_KzHSQQbtB",
        "outputId": "bde9f42f-a565-41ca-8dbe-95ba7141d838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.withColumn('hr_avg', (df.hr_01+df.hr_02)/2).show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|hr_avg|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+------+\n",
            "|2015/01/01| 46.0| 71.0| 76.0| 74.0| 65.0| 62.0| 56.0| 50.0| 52.0| 56.0| 54.0| 47.0| 40.0| 36.0| 37.0| 27.0| 30.0| 25.0| 26.0| 24.0| 18.0| 16.0| 11.0| 14.0|      龍潭|  PM2.5|  58.5|\n",
            "|2015/01/02| 15.0| 12.0|  9.0| 14.0| 17.0| 20.0| 18.0| 22.0| 21.0| 23.0| 18.0| 25.0| 24.0| 27.0| 18.0| 23.0| 18.0| 19.0| 18.0| 21.0| 23.0| 18.0| 19.0| 19.0|      龍潭|  PM2.5|  13.5|\n",
            "|2015/01/03| 14.0| 14.0|  9.0| 13.0| 12.0| 15.0|  9.0| 11.0| 14.0| 27.0| 27.0| 24.0| 16.0| 27.0| 38.0| 39.0| 35.0| 32.0| 38.0| 36.0| 34.0| 36.0| 36.0| 39.0|      龍潭|  PM2.5|  14.0|\n",
            "|2015/01/04| 42.0| 40.0| 39.0| 34.0| 33.0| 25.0| 24.0| 23.0| 31.0| 27.0| 34.0| 35.0| 35.0| 38.0| 46.0| 59.0| 53.0| 58.0| 61.0| 74.0| 80.0| 78.0| 70.0| 53.0|      龍潭|  PM2.5|  41.0|\n",
            "|2015/01/05| 42.0| 33.0| 24.0| 21.0| 20.0| 18.0| 16.0| 21.0| 21.0| 17.0| 13.0| 23.0| 36.0| 49.0| 61.0| 63.0| 68.0| 81.0| 91.0| 99.0| 76.0| 57.0| 34.0| 24.0|      龍潭|  PM2.5|  37.5|\n",
            "|2015/01/06| 19.0| 13.0| 11.0| 16.0| 11.0|  9.0|  6.0|  8.0|  8.0|  7.0| 11.0| 19.0| 33.0| 32.0| 34.0| 30.0| 32.0| 39.0| 53.0| 73.0| 68.0| 61.0| 36.0| 28.0|      龍潭|  PM2.5|  16.0|\n",
            "|2015/01/08| 22.0| 30.0| 28.0| 37.0| 34.0| 40.0| 34.0| 41.0| 41.0| 50.0| 54.0| 51.0| 48.0| 43.0| 27.0| 26.0| 39.0| 45.0| 40.0| 42.0| 39.0| 32.0| 24.0| 25.0|      龍潭|  PM2.5|  26.0|\n",
            "|2015/01/09| 23.0| 23.0| 17.0| 15.0| 13.0| 14.0| 18.0| 26.0| 27.0| 30.0| 25.0| 23.0| 18.0| 17.0| 26.0| 26.0| 36.0| 29.0| 36.0| 26.0| 30.0| 29.0| 27.0| 22.0|      龍潭|  PM2.5|  23.0|\n",
            "|2015/01/10| 20.0| 18.0| 13.0|  9.0| 13.0| 11.0| 11.0| 10.0| 13.0| 17.0| 15.0| 20.0| 10.0| 18.0| 14.0| 17.0| 14.0| 13.0| 16.0| 20.0| 27.0| 31.0| 27.0| 28.0|      龍潭|  PM2.5|  19.0|\n",
            "|2015/01/11| 31.0| 30.0| 32.0| 30.0| 33.0| 33.0| 32.0| 35.0| 26.0| 25.0| 18.0| 25.0| 22.0| 26.0| 25.0| 29.0| 24.0| 25.0| 24.0| 33.0| 38.0| 43.0| 41.0| 33.0|      龍潭|  PM2.5|  30.5|\n",
            "|2015/01/13| 11.0| 12.0| 11.0| 13.0| 16.0| 11.0| 14.0|  8.0| 10.0| 10.0| 22.0| 25.0| 31.0| 31.0| 43.0| 36.0| 43.0| 34.0| 37.0| 25.0| 20.0| 14.0|  8.0|  8.0|      龍潭|  PM2.5|  11.5|\n",
            "|2015/01/14|  3.0|  5.0|  5.0|  5.0|  3.0|  4.0|  5.0|  7.0|  9.0| 11.0| 17.0| 10.0| 13.0|  6.0|  6.0| 12.0| 16.0| 23.0| 16.0| 15.0| 12.0| 12.0| 14.0| 12.0|      龍潭|  PM2.5|   4.0|\n",
            "|2015/01/15| 14.0|  8.0| 11.0| 13.0| 20.0| 23.0| 23.0| 21.0| 17.0| 19.0| 20.0| 23.0| 20.0| 24.0| 23.0| 18.0| 13.0| 13.0| 14.0| 19.0| 14.0| 16.0|  8.0|  8.0|      龍潭|  PM2.5|  11.0|\n",
            "|2015/01/16|  8.0| 14.0| 15.0| 10.0|  8.0|  6.0| 11.0| 11.0| 21.0| 20.0| 25.0| 24.0| 22.0| 24.0| 19.0| 31.0| 18.0| 32.0| 31.0| 47.0| 38.0| 36.0| 25.0| 25.0|      龍潭|  PM2.5|  11.0|\n",
            "|2015/01/17| 23.0| 33.0| 31.0| 30.0| 35.0| 37.0| 50.0| 57.0| 78.0| 77.0| 72.0| 59.0| 63.0| 61.0| 67.0| 61.0| 64.0| 65.0| 71.0| 63.0| 69.0| 75.0| 79.0| 73.0|      龍潭|  PM2.5|  28.0|\n",
            "|2015/01/18| 55.0| 51.0| 40.0| 39.0| 32.0| 31.0| 28.0| 27.0| 27.0| 31.0| 30.0| 20.0| 17.0| 17.0| 23.0| 15.0| 14.0|  8.0| 15.0| 24.0| 22.0| 22.0| 13.0| 19.0|      龍潭|  PM2.5|  53.0|\n",
            "|2015/01/19| 12.0| 11.0| 11.0| 18.0| 22.0| 22.0| 22.0| 25.0| 26.0| 39.0| 51.0| 51.0| 43.0| 42.0| 46.0| 41.0| 34.0| 37.0| 40.0| 39.0| 37.0| 36.0| 32.0| 34.0|      龍潭|  PM2.5|  11.5|\n",
            "|2015/01/20| 34.0| 41.0| 35.0| 34.0| 36.0| 38.0| 38.0| 36.0| 38.0| 39.0| 38.0| 38.0| 36.0| 38.0| 38.0| 37.0| 38.0| 33.0| 37.0| 35.0| 46.0| 32.0| 28.0| 29.0|      龍潭|  PM2.5|  37.5|\n",
            "|2015/01/21| 34.0| 39.0| 33.0| 43.0| 46.0| 49.0| 40.0| 28.0| 27.0| 21.0| 27.0| 24.0| 29.0| 35.0| 32.0| 38.0| 37.0| 45.0| 42.0| 40.0| 41.0| 43.0| 47.0| 40.0|      龍潭|  PM2.5|  36.5|\n",
            "|2015/01/22| 36.0| 34.0| 30.0| 33.0| 27.0| 28.0| 23.0| 26.0| 26.0| 27.0| 28.0| 38.0| 49.0| 58.0| 68.0| 83.0| 87.0| 83.0| 71.0| 70.0| 76.0| 73.0| 70.0| 59.0|      龍潭|  PM2.5|  35.0|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyEnb1kZ2CD"
      },
      "source": [
        "# 練習1: 請用SPARK SQL算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kti4CDbZ2CC"
      },
      "source": [
        "# 練習2: 請使用SPARK SQL求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwEAiWd7Xme"
      },
      "source": [
        "df = clean_weather_data.toDF()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPe7rk3O7Xmd",
        "outputId": "87336f1b-fdd3-4f77-ea6d-1ade51ca87bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.filter(df[\"_3\"]==\"PM2.5\").show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|        _1| _2|   _3| _4| _5| _6| _7| _8| _9|_10|_11|_12|_13|_14|_15|_16|_17|_18|_19|_20|_21|_22|_23|_24|_25|_26|_27|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "|2015/01/01| 龍潭|PM2.5| 46| 71| 76| 74| 65| 62| 56| 50| 52| 56| 54| 47| 40| 36| 37| 27| 30| 25| 26| 24| 18| 16| 11| 14|\n",
            "|2015/01/02| 龍潭|PM2.5| 15| 12|  9| 14| 17| 20| 18| 22| 21| 23| 18| 25| 24| 27| 18| 23| 18| 19| 18| 21| 23| 18| 19| 19|\n",
            "|2015/01/03| 龍潭|PM2.5| 14| 14|  9| 13| 12| 15|  9| 11| 14| 27| 27| 24| 16| 27| 38| 39| 35| 32| 38| 36| 34| 36| 36| 39|\n",
            "|2015/01/04| 龍潭|PM2.5| 42| 40| 39| 34| 33| 25| 24| 23| 31| 27| 34| 35| 35| 38| 46| 59| 53| 58| 61| 74| 80| 78| 70| 53|\n",
            "|2015/01/05| 龍潭|PM2.5| 42| 33| 24| 21| 20| 18| 16| 21| 21| 17| 13| 23| 36| 49| 61| 63| 68| 81| 91| 99| 76| 57| 34| 24|\n",
            "|2015/01/06| 龍潭|PM2.5| 19| 13| 11| 16| 11|  9|  6|  8|  8|  7| 11| 19| 33| 32| 34| 30| 32| 39| 53| 73| 68| 61| 36| 28|\n",
            "|2015/01/08| 龍潭|PM2.5| 22| 30| 28| 37| 34| 40| 34| 41| 41| 50| 54| 51| 48| 43| 27| 26| 39| 45| 40| 42| 39| 32| 24| 25|\n",
            "|2015/01/09| 龍潭|PM2.5| 23| 23| 17| 15| 13| 14| 18| 26| 27| 30| 25| 23| 18| 17| 26| 26| 36| 29| 36| 26| 30| 29| 27| 22|\n",
            "|2015/01/10| 龍潭|PM2.5| 20| 18| 13|  9| 13| 11| 11| 10| 13| 17| 15| 20| 10| 18| 14| 17| 14| 13| 16| 20| 27| 31| 27| 28|\n",
            "|2015/01/11| 龍潭|PM2.5| 31| 30| 32| 30| 33| 33| 32| 35| 26| 25| 18| 25| 22| 26| 25| 29| 24| 25| 24| 33| 38| 43| 41| 33|\n",
            "|2015/01/13| 龍潭|PM2.5| 11| 12| 11| 13| 16| 11| 14|  8| 10| 10| 22| 25| 31| 31| 43| 36| 43| 34| 37| 25| 20| 14|  8|  8|\n",
            "|2015/01/14| 龍潭|PM2.5|  3|  5|  5|  5|  3|  4|  5|  7|  9| 11| 17| 10| 13|  6|  6| 12| 16| 23| 16| 15| 12| 12| 14| 12|\n",
            "|2015/01/15| 龍潭|PM2.5| 14|  8| 11| 13| 20| 23| 23| 21| 17| 19| 20| 23| 20| 24| 23| 18| 13| 13| 14| 19| 14| 16|  8|  8|\n",
            "|2015/01/16| 龍潭|PM2.5|  8| 14| 15| 10|  8|  6| 11| 11| 21| 20| 25| 24| 22| 24| 19| 31| 18| 32| 31| 47| 38| 36| 25| 25|\n",
            "|2015/01/17| 龍潭|PM2.5| 23| 33| 31| 30| 35| 37| 50| 57| 78| 77| 72| 59| 63| 61| 67| 61| 64| 65| 71| 63| 69| 75| 79| 73|\n",
            "|2015/01/18| 龍潭|PM2.5| 55| 51| 40| 39| 32| 31| 28| 27| 27| 31| 30| 20| 17| 17| 23| 15| 14|  8| 15| 24| 22| 22| 13| 19|\n",
            "|2015/01/19| 龍潭|PM2.5| 12| 11| 11| 18| 22| 22| 22| 25| 26| 39| 51| 51| 43| 42| 46| 41| 34| 37| 40| 39| 37| 36| 32| 34|\n",
            "|2015/01/20| 龍潭|PM2.5| 34| 41| 35| 34| 36| 38| 38| 36| 38| 39| 38| 38| 36| 38| 38| 37| 38| 33| 37| 35| 46| 32| 28| 29|\n",
            "|2015/01/21| 龍潭|PM2.5| 34| 39| 33| 43| 46| 49| 40| 28| 27| 21| 27| 24| 29| 35| 32| 38| 37| 45| 42| 40| 41| 43| 47| 40|\n",
            "|2015/01/22| 龍潭|PM2.5| 36| 34| 30| 33| 27| 28| 23| 26| 26| 27| 28| 38| 49| 58| 68| 83| 87| 83| 71| 70| 76| 73| 70| 59|\n",
            "+----------+---+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI0pFyRK7XmY"
      },
      "source": [
        "for i in df.columns[3:]:\n",
        "  df = df.withColumn(i, df[i].cast(\"double\"))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lygubp5Y7XmX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-6goC4loJ0D"
      },
      "source": [
        "avgdf = df.filter(df[\"_3\"]==\"PM2.5\").withColumn(\"dayavg\", \n",
        "                                        (\n",
        "                                        df[\"_4\"]+df[\"_5\"]+df[\"_6\"]+df[\"_7\"]+df[\"_8\"]+df[\"_9\"]+df[\"_10\"]+df[\"_11\"]\\\n",
        "                                        +df[\"_12\"]+df[\"_13\"]+df[\"_14\"]+df[\"_15\"]+df[\"_16\"]+df[\"_17\"]+df[\"_18\"]+df[\"_19\"]\\\n",
        "                                        +df[\"_20\"]+df[\"_21\"]+df[\"_22\"]+df[\"_23\"]+df[\"_24\"]+df[\"_25\"]+df[\"_26\"]+df[\"_27\"])/24)\\\n",
        "                                        .select(\"_1\", \"_2\",\"dayavg\")"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV1IM3QRoT9q",
        "outputId": "cfb63748-b691-4d01-ee18-c745955bbf47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "avgdf.show()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+------------------+\n",
            "|        _1| _2|            dayavg|\n",
            "+----------+---+------------------+\n",
            "|2015/01/01| 龍潭|42.208333333333336|\n",
            "|2015/01/02| 龍潭|19.208333333333332|\n",
            "|2015/01/03| 龍潭|24.791666666666668|\n",
            "|2015/01/04| 龍潭|              45.5|\n",
            "|2015/01/05| 龍潭|              42.0|\n",
            "|2015/01/06| 龍潭|            27.375|\n",
            "|2015/01/08| 龍潭|37.166666666666664|\n",
            "|2015/01/09| 龍潭|              24.0|\n",
            "|2015/01/10| 龍潭|            16.875|\n",
            "|2015/01/11| 龍潭|29.708333333333332|\n",
            "|2015/01/13| 龍潭|20.541666666666668|\n",
            "|2015/01/14| 龍潭|10.041666666666666|\n",
            "|2015/01/15| 龍潭|             16.75|\n",
            "|2015/01/16| 龍潭|21.708333333333332|\n",
            "|2015/01/17| 龍潭|58.041666666666664|\n",
            "|2015/01/18| 龍潭|25.833333333333332|\n",
            "|2015/01/19| 龍潭|            32.125|\n",
            "|2015/01/20| 龍潭|36.333333333333336|\n",
            "|2015/01/21| 龍潭|36.666666666666664|\n",
            "|2015/01/22| 龍潭|            50.125|\n",
            "+----------+---+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJZ_xlp82c5F",
        "outputId": "a5fc4130-1eb6-4ff3-e0ab-31f1ee96226f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60).show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+---+------------------+\n",
            "|        _1| _2|            dayavg|\n",
            "+----------+---+------------------+\n",
            "|2015/02/05| 龍潭| 68.95833333333333|\n",
            "|2015/12/16| 龍潭| 74.54166666666667|\n",
            "|2015/01/04| 麥寮|60.166666666666664|\n",
            "|2015/01/05| 麥寮|              79.5|\n",
            "|2015/01/17| 麥寮| 65.29166666666667|\n",
            "|2015/01/24| 麥寮| 80.91666666666667|\n",
            "|2015/01/25| 麥寮| 71.58333333333333|\n",
            "|2015/02/05| 麥寮| 75.66666666666667|\n",
            "|2015/02/14| 麥寮|61.541666666666664|\n",
            "|2015/02/15| 麥寮| 75.29166666666667|\n",
            "|2015/03/14| 麥寮|             63.75|\n",
            "|2015/03/15| 麥寮| 74.33333333333333|\n",
            "|2015/03/20| 麥寮| 67.79166666666667|\n",
            "|2015/12/16| 麥寮|             71.25|\n",
            "|2015/01/17| 鳳山|             61.25|\n",
            "|2015/01/18| 鳳山|60.458333333333336|\n",
            "|2015/01/25| 鳳山|              60.5|\n",
            "|2015/01/26| 鳳山| 67.70833333333333|\n",
            "|2015/02/05| 鳳山|             62.75|\n",
            "|2015/02/06| 鳳山| 79.04166666666667|\n",
            "+----------+---+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMRbvkL-2PuG",
        "outputId": "c0f29688-2cc2-4063-e854-8f820b0cf90e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60).groupby(avgdf._2).count().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+\n",
            "| _2|count|\n",
            "+---+-----+\n",
            "| 頭份|    4|\n",
            "| 金門|   21|\n",
            "| 新港|    8|\n",
            "| 崙背|   23|\n",
            "| 彰化|    6|\n",
            "| 龍潭|    2|\n",
            "| 新竹|    1|\n",
            "| 竹山|   28|\n",
            "| 松山|    2|\n",
            "| 善化|   18|\n",
            "| 楠梓|   13|\n",
            "| 竹東|    2|\n",
            "| 土城|    1|\n",
            "| 小港|   11|\n",
            "| 臺西|   10|\n",
            "| 潮州|    7|\n",
            "| 苗栗|    3|\n",
            "| 仁武|   10|\n",
            "| 馬祖|    9|\n",
            "| 西屯|    9|\n",
            "+---+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDq8ZLtU7Xl3",
        "outputId": "d15c8e91-e99b-45fc-de53-f1f0a242f5f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60)\\\n",
        "     .groupby(avgdf._2)\\\n",
        "     .count()\\\n",
        "     .orderBy('count', ascending=False)\\\n",
        "     .show()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-66bd857e9020>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    avgdf.filter(avgdf.dayavg>60)\\.groupby(avgdf._2).count().orderBy('count', ascending=False).show()\u001b[0m\n\u001b[0m                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUg4ZNDN3n4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycnCvcAVZ2Bi"
      },
      "source": [
        "# 使用 Spark SQL來下達SQL查詢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfO7cTEUZ2Bj",
        "outputId": "662a1cde-c130-4853-8dda-7309da9120aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()\n",
        "df.registerTempTable(\"Table\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/01/01| 46.0| 71.0| 76.0| 74.0| 65.0| 62.0| 56.0| 50.0| 52.0| 56.0| 54.0| 47.0| 40.0| 36.0| 37.0| 27.0| 30.0| 25.0| 26.0| 24.0| 18.0| 16.0| 11.0| 14.0|      龍潭|  PM2.5|\n",
            "|2015/01/02| 15.0| 12.0|  9.0| 14.0| 17.0| 20.0| 18.0| 22.0| 21.0| 23.0| 18.0| 25.0| 24.0| 27.0| 18.0| 23.0| 18.0| 19.0| 18.0| 21.0| 23.0| 18.0| 19.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/03| 14.0| 14.0|  9.0| 13.0| 12.0| 15.0|  9.0| 11.0| 14.0| 27.0| 27.0| 24.0| 16.0| 27.0| 38.0| 39.0| 35.0| 32.0| 38.0| 36.0| 34.0| 36.0| 36.0| 39.0|      龍潭|  PM2.5|\n",
            "|2015/01/04| 42.0| 40.0| 39.0| 34.0| 33.0| 25.0| 24.0| 23.0| 31.0| 27.0| 34.0| 35.0| 35.0| 38.0| 46.0| 59.0| 53.0| 58.0| 61.0| 74.0| 80.0| 78.0| 70.0| 53.0|      龍潭|  PM2.5|\n",
            "|2015/01/05| 42.0| 33.0| 24.0| 21.0| 20.0| 18.0| 16.0| 21.0| 21.0| 17.0| 13.0| 23.0| 36.0| 49.0| 61.0| 63.0| 68.0| 81.0| 91.0| 99.0| 76.0| 57.0| 34.0| 24.0|      龍潭|  PM2.5|\n",
            "|2015/01/06| 19.0| 13.0| 11.0| 16.0| 11.0|  9.0|  6.0|  8.0|  8.0|  7.0| 11.0| 19.0| 33.0| 32.0| 34.0| 30.0| 32.0| 39.0| 53.0| 73.0| 68.0| 61.0| 36.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/08| 22.0| 30.0| 28.0| 37.0| 34.0| 40.0| 34.0| 41.0| 41.0| 50.0| 54.0| 51.0| 48.0| 43.0| 27.0| 26.0| 39.0| 45.0| 40.0| 42.0| 39.0| 32.0| 24.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/09| 23.0| 23.0| 17.0| 15.0| 13.0| 14.0| 18.0| 26.0| 27.0| 30.0| 25.0| 23.0| 18.0| 17.0| 26.0| 26.0| 36.0| 29.0| 36.0| 26.0| 30.0| 29.0| 27.0| 22.0|      龍潭|  PM2.5|\n",
            "|2015/01/10| 20.0| 18.0| 13.0|  9.0| 13.0| 11.0| 11.0| 10.0| 13.0| 17.0| 15.0| 20.0| 10.0| 18.0| 14.0| 17.0| 14.0| 13.0| 16.0| 20.0| 27.0| 31.0| 27.0| 28.0|      龍潭|  PM2.5|\n",
            "|2015/01/11| 31.0| 30.0| 32.0| 30.0| 33.0| 33.0| 32.0| 35.0| 26.0| 25.0| 18.0| 25.0| 22.0| 26.0| 25.0| 29.0| 24.0| 25.0| 24.0| 33.0| 38.0| 43.0| 41.0| 33.0|      龍潭|  PM2.5|\n",
            "|2015/01/13| 11.0| 12.0| 11.0| 13.0| 16.0| 11.0| 14.0|  8.0| 10.0| 10.0| 22.0| 25.0| 31.0| 31.0| 43.0| 36.0| 43.0| 34.0| 37.0| 25.0| 20.0| 14.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/14|  3.0|  5.0|  5.0|  5.0|  3.0|  4.0|  5.0|  7.0|  9.0| 11.0| 17.0| 10.0| 13.0|  6.0|  6.0| 12.0| 16.0| 23.0| 16.0| 15.0| 12.0| 12.0| 14.0| 12.0|      龍潭|  PM2.5|\n",
            "|2015/01/15| 14.0|  8.0| 11.0| 13.0| 20.0| 23.0| 23.0| 21.0| 17.0| 19.0| 20.0| 23.0| 20.0| 24.0| 23.0| 18.0| 13.0| 13.0| 14.0| 19.0| 14.0| 16.0|  8.0|  8.0|      龍潭|  PM2.5|\n",
            "|2015/01/16|  8.0| 14.0| 15.0| 10.0|  8.0|  6.0| 11.0| 11.0| 21.0| 20.0| 25.0| 24.0| 22.0| 24.0| 19.0| 31.0| 18.0| 32.0| 31.0| 47.0| 38.0| 36.0| 25.0| 25.0|      龍潭|  PM2.5|\n",
            "|2015/01/17| 23.0| 33.0| 31.0| 30.0| 35.0| 37.0| 50.0| 57.0| 78.0| 77.0| 72.0| 59.0| 63.0| 61.0| 67.0| 61.0| 64.0| 65.0| 71.0| 63.0| 69.0| 75.0| 79.0| 73.0|      龍潭|  PM2.5|\n",
            "|2015/01/18| 55.0| 51.0| 40.0| 39.0| 32.0| 31.0| 28.0| 27.0| 27.0| 31.0| 30.0| 20.0| 17.0| 17.0| 23.0| 15.0| 14.0|  8.0| 15.0| 24.0| 22.0| 22.0| 13.0| 19.0|      龍潭|  PM2.5|\n",
            "|2015/01/19| 12.0| 11.0| 11.0| 18.0| 22.0| 22.0| 22.0| 25.0| 26.0| 39.0| 51.0| 51.0| 43.0| 42.0| 46.0| 41.0| 34.0| 37.0| 40.0| 39.0| 37.0| 36.0| 32.0| 34.0|      龍潭|  PM2.5|\n",
            "|2015/01/20| 34.0| 41.0| 35.0| 34.0| 36.0| 38.0| 38.0| 36.0| 38.0| 39.0| 38.0| 38.0| 36.0| 38.0| 38.0| 37.0| 38.0| 33.0| 37.0| 35.0| 46.0| 32.0| 28.0| 29.0|      龍潭|  PM2.5|\n",
            "|2015/01/21| 34.0| 39.0| 33.0| 43.0| 46.0| 49.0| 40.0| 28.0| 27.0| 21.0| 27.0| 24.0| 29.0| 35.0| 32.0| 38.0| 37.0| 45.0| 42.0| 40.0| 41.0| 43.0| 47.0| 40.0|      龍潭|  PM2.5|\n",
            "|2015/01/22| 36.0| 34.0| 30.0| 33.0| 27.0| 28.0| 23.0| 26.0| 26.0| 27.0| 28.0| 38.0| 49.0| 58.0| 68.0| 83.0| 87.0| 83.0| 71.0| 70.0| 76.0| 73.0| 70.0| 59.0|      龍潭|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSsE-3AWZ2Bl",
        "outputId": "e238ad59-a8c6-4373-c3e4-21e456f960a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "            select * from Table where date ='2015/02/07'\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|      date|hr_01|hr_02|hr_03|hr_04|hr_05|hr_06|hr_07|hr_08|hr_09|hr_10|hr_11|hr_12|hr_13|hr_14|hr_15|hr_16|hr_17|hr_18|hr_19|hr_20|hr_21|hr_22|hr_23|hr_24|location|measure|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "|2015/02/07| 14.0| 16.0| 15.0|  8.0|  7.0|  9.0| 10.0| 13.0| 17.0| 15.0| 18.0| 15.0| 32.0| 33.0| 36.0| 20.0| 18.0| 17.0| 25.0| 25.0| 22.0| 18.0| 17.0| 16.0|      龍潭|  PM2.5|\n",
            "|2015/02/07| 31.0| 30.0| 36.0| 39.0| 35.0| 35.0| 35.0| 35.0| 34.0| 36.0| 31.0| 31.0| 49.0| 56.0| 44.0| 37.0| 40.0| 35.0| 30.0| 29.0| 24.0| 24.0| 22.0| 22.0|      麥寮|  PM2.5|\n",
            "|2015/02/07| 41.0| 44.0| 43.0| 51.0| 49.0| 59.0| 69.0| 75.0| 75.0| 75.0| 80.0| 76.0| 75.0| 68.0| 55.0| 53.0| 39.0| 46.0| 36.0| 48.0| 53.0| 58.0| 60.0| 50.0|      鳳山|  PM2.5|\n",
            "|2015/02/07| 33.0| 31.0| 29.0| 35.0| 36.0| 40.0| 47.0| 48.0| 47.0| 48.0| 52.0| 47.0| 45.0| 47.0| 40.0| 37.0| 37.0| 34.0| 30.0| 30.0| 35.0| 34.0| 29.0| 27.0|      馬祖|  PM2.5|\n",
            "|2015/02/07| 20.0| 22.0| 24.0| 21.0| 25.0| 34.0| 33.0| 31.0| 30.0| 27.0| 30.0| 34.0| 30.0| 31.0| 35.0| 36.0| 34.0| 33.0| 35.0| 33.0| 30.0| 27.0| 25.0| 27.0|      馬公|  PM2.5|\n",
            "|2015/02/07| 26.0| 30.0| 30.0| 27.0| 24.0| 19.0| 20.0| 18.0| 17.0| 27.0| 37.0| 42.0| 33.0| 20.0| 13.0| 15.0| 16.0| 14.0| 15.0| 16.0| 17.0| 17.0| 19.0| 19.0|      頭份|  PM2.5|\n",
            "|2015/02/07|  1.0|  1.0|  2.0|  2.0|  4.0|  2.0|  1.0|  1.0|  4.0|  7.0| 13.0|  8.0|  9.0| 13.0| 16.0| 12.0| 12.0|  8.0| 12.0|  8.0|  0.0|  0.0|  0.0|  0.0|      陽明|  PM2.5|\n",
            "|2015/02/07| 20.0| 17.0| 15.0| 16.0| 17.0| 17.0| 18.0| 17.0| 12.0| 12.0| 11.0| 10.0|  9.0|  9.0|  9.0|  6.0|  7.0|  7.0|  9.0| 12.0| 12.0| 12.0| 12.0| 12.0|      關山|  PM2.5|\n",
            "|2015/02/07| 45.0| 45.0| 47.0| 47.0| 41.0| 42.0| 47.0| 57.0| 65.0| 49.0| 40.0| 42.0| 39.0| 35.0| 36.0| 36.0| 38.0| 38.0| 36.0| 37.0| 32.0| 29.0| 30.0| 31.0|      金門|  PM2.5|\n",
            "|2015/02/07|  9.0| 11.0| 10.0| 11.0|  9.0| 10.0| 10.0| 11.0| 14.0| 15.0| 13.0| 12.0| 14.0| 17.0| 16.0| 14.0| 14.0| 13.0| 16.0| 17.0| 14.0| 11.0| 13.0| 14.0|      觀音|  PM2.5|\n",
            "|2015/02/07| 26.0| 22.0| 20.0| 22.0| 26.0| 28.0| 38.0| 43.0| 49.0| 59.0| 58.0| 50.0| 38.0| 41.0| 40.0| 30.0| 27.0| 26.0| 33.0| 26.0| 30.0| 30.0| 36.0| 30.0|      西屯|  PM2.5|\n",
            "|2015/02/07| 10.0|  9.0|  9.0|  7.0|  9.0|  7.0|  6.0|  7.0|  6.0|  6.0|  7.0|  7.0|  8.0|  6.0|  3.0|  4.0|  6.0|  9.0| 13.0| 13.0| 13.0| 17.0| 18.0| 18.0|      萬里|  PM2.5|\n",
            "|2015/02/07| 10.0|  3.0|  8.0|  5.0|  5.0|  1.0|  1.0|  2.0|  0.0|  9.0| 15.0| 31.0| 21.0| 16.0| 13.0| 17.0| 22.0| 17.0| 23.0| 29.0| 31.0| 28.0| 21.0| 16.0|      萬華|  PM2.5|\n",
            "|2015/02/07| 19.0| 18.0| 14.0| 14.0| 14.0| 13.0| 12.0| 16.0| 21.0| 26.0| 32.0| 37.0| 26.0| 16.0| 24.0| 22.0| 14.0| 18.0| 21.0| 22.0| 29.0| 26.0| 18.0| 18.0|      菜寮|  PM2.5|\n",
            "|2015/02/07| 24.0| 26.0| 29.0| 30.0| 29.0| 27.0| 28.0| 30.0| 33.0| 36.0| 36.0| 37.0| 37.0| 34.0| 24.0| 19.0| 25.0| 28.0| 22.0| 18.0| 24.0| 31.0| 35.0| 33.0|      苗栗|  PM2.5|\n",
            "|2015/02/07| 10.0| 10.0|  7.0|  8.0|  9.0|  8.0|  4.0|  4.0|  6.0|  7.0|  8.0|  3.0|  0.0|  0.0|  0.0|  0.0|  0.0|  5.0|  7.0|  9.0| 12.0| 11.0|  5.0|  1.0|      花蓮|  PM2.5|\n",
            "|2015/02/07| 31.0| 26.0| 32.0| 38.0| 38.0| 38.0| 38.0| 40.0| 36.0| 31.0| 23.0| 18.0| 40.0| 55.0| 47.0| 43.0| 38.0| 32.0| 30.0| 29.0| 23.0| 19.0| 17.0| 15.0|      臺西|  PM2.5|\n",
            "|2015/02/07| 10.0| 11.0| 15.0| 17.0| 16.0| 12.0| 11.0| 17.0| 14.0| 20.0| 17.0|  3.0|  6.0|  9.0|  9.0|  9.0|  9.0|  9.0|  9.0| 13.0| 16.0| 14.0| 11.0| 10.0|      臺東|  PM2.5|\n",
            "|2015/02/07| 33.0| 33.0| 40.0| 36.0| 37.0| 35.0| 34.0| 36.0| 57.0| 64.0| 71.0| 59.0| 58.0| 60.0| 51.0| 47.0| 35.0| 39.0| 43.0| 43.0| 41.0| 36.0| 32.0| 26.0|      臺南|  PM2.5|\n",
            "|2015/02/07| 61.0| 61.0| 53.0| 56.0| 54.0| 53.0| 56.0| 57.0| 63.0| 75.0| 78.0| 51.0| 35.0| 21.0| 16.0| 27.0| 47.0| 53.0| 49.0| 61.0| 56.0| 51.0| 56.0| 57.0|      美濃|  PM2.5|\n",
            "+----------+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+--------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vVEpvUZ2Bo"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "                select count(*) count \n",
        "                from Table \n",
        "                where hr_01 > 100\n",
        "               \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxauml1-Z2Bt",
        "outputId": "b985b479-3b82-4c6d-fbcf-4825e08c97ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDPUeJQp6Ej_",
        "outputId": "580884c1-4407-4325-e179-2c12b0a17720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date, location \n",
        "                from Table\n",
        "                where date=\"2015/02/07\"\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+\n",
            "|      date|location|\n",
            "+----------+--------+\n",
            "|2015/02/07|      龍潭|\n",
            "|2015/02/07|      麥寮|\n",
            "|2015/02/07|      鳳山|\n",
            "|2015/02/07|      馬祖|\n",
            "|2015/02/07|      馬公|\n",
            "|2015/02/07|      頭份|\n",
            "|2015/02/07|      陽明|\n",
            "|2015/02/07|      關山|\n",
            "|2015/02/07|      金門|\n",
            "|2015/02/07|      觀音|\n",
            "|2015/02/07|      西屯|\n",
            "|2015/02/07|      萬里|\n",
            "|2015/02/07|      萬華|\n",
            "|2015/02/07|      菜寮|\n",
            "|2015/02/07|      苗栗|\n",
            "|2015/02/07|      花蓮|\n",
            "|2015/02/07|      臺西|\n",
            "|2015/02/07|      臺東|\n",
            "|2015/02/07|      臺南|\n",
            "|2015/02/07|      美濃|\n",
            "+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jux8542WZ2Bv",
        "outputId": "5998e3b8-8600-46d9-81a9-eea249f5f91e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n",
        "                from Table\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+--------+-----+-----+-----+\n",
            "|      date|location|hr_01|hr_02| diff|\n",
            "+----------+--------+-----+-----+-----+\n",
            "|2015/01/01|      龍潭| 46.0| 71.0|-25.0|\n",
            "|2015/01/02|      龍潭| 15.0| 12.0|  3.0|\n",
            "|2015/01/03|      龍潭| 14.0| 14.0|  0.0|\n",
            "|2015/01/04|      龍潭| 42.0| 40.0|  2.0|\n",
            "|2015/01/05|      龍潭| 42.0| 33.0|  9.0|\n",
            "|2015/01/06|      龍潭| 19.0| 13.0|  6.0|\n",
            "|2015/01/08|      龍潭| 22.0| 30.0| -8.0|\n",
            "|2015/01/09|      龍潭| 23.0| 23.0|  0.0|\n",
            "|2015/01/10|      龍潭| 20.0| 18.0|  2.0|\n",
            "|2015/01/11|      龍潭| 31.0| 30.0|  1.0|\n",
            "|2015/01/13|      龍潭| 11.0| 12.0| -1.0|\n",
            "|2015/01/14|      龍潭|  3.0|  5.0| -2.0|\n",
            "|2015/01/15|      龍潭| 14.0|  8.0|  6.0|\n",
            "|2015/01/16|      龍潭|  8.0| 14.0| -6.0|\n",
            "|2015/01/17|      龍潭| 23.0| 33.0|-10.0|\n",
            "|2015/01/18|      龍潭| 55.0| 51.0|  4.0|\n",
            "|2015/01/19|      龍潭| 12.0| 11.0|  1.0|\n",
            "|2015/01/20|      龍潭| 34.0| 41.0| -7.0|\n",
            "|2015/01/21|      龍潭| 34.0| 39.0| -5.0|\n",
            "|2015/01/22|      龍潭| 36.0| 34.0|  2.0|\n",
            "+----------+--------+-----+-----+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQg8tWY8ty-B"
      },
      "source": [
        "## 練習1: 請使用SparkSQL，算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6yIB8oitx1f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKjn6PZ2Bw",
        "outputId": "aefaf163-7bbd-48d0-ea7e-01aaeb01c993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "expr=\"\"\n",
        "for i in df.columns[1:25]:\n",
        "  expr=str(i)+\"+\"+expr\n",
        "\n",
        "print(expr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hr_24+hr_23+hr_22+hr_21+hr_20+hr_19+hr_18+hr_17+hr_16+hr_15+hr_14+hr_13+hr_12+hr_11+hr_10+hr_09+hr_08+hr_07+hr_06+hr_05+hr_04+hr_03+hr_02+hr_01+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzr5S_ysZ2By"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select location, date, (hr_24+hr_23+hr_22+hr_21+hr_20+hr_19+hr_18+hr_17+hr_16+hr_15+hr_14+hr_13+hr_12+hr_11+hr_10+hr_09+hr_08+hr_07+hr_06+hr_05+hr_04+hr_03+hr_02+hr_01)/24 as avg\n",
        "                from Table \n",
        "                having avg>60\n",
        "          \"\"\").registerTempTable(\"DayAvgTable\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVcomNQfZ2B0",
        "outputId": "bbbb79ee-4dcd-4957-84cb-72afea83e25d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "          select location, count(date) as count\n",
        "          from DayAvgTable\n",
        "          group by location\n",
        "          order by count DESC\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|location|count|\n",
            "+--------+-----+\n",
            "|      竹山|   28|\n",
            "|      斗六|   26|\n",
            "|      崙背|   23|\n",
            "|      金門|   21|\n",
            "|      善化|   18|\n",
            "|      埔里|   16|\n",
            "|      嘉義|   14|\n",
            "|      橋頭|   14|\n",
            "|      左營|   14|\n",
            "|      復興|   13|\n",
            "|      楠梓|   13|\n",
            "|      麥寮|   12|\n",
            "|      小港|   11|\n",
            "|      大寮|   11|\n",
            "|      鳳山|   11|\n",
            "|      忠明|   11|\n",
            "|      臺西|   10|\n",
            "|      仁武|   10|\n",
            "|      大里|   10|\n",
            "|      二林|   10|\n",
            "+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HryYFAWXZ2B1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMpQ-zrB11pp"
      },
      "source": [
        "# 利用DataFrame 直接讀取CSV, Fillna, DropNa功能清洗資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO_EEY1011pq"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pDYpUS811ps"
      },
      "source": [
        "df = spark.read.load(\"./pm25.csv\", format=\"csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBn5I6iY11pt"
      },
      "source": [
        "df.show(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Du-yrk11pw"
      },
      "source": [
        "df2=df.filter(df[\"_c0\"]!=\"日期\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRUGtKK11py"
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcL55Mg211p0"
      },
      "source": [
        "df2.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CN4CwP_11p2"
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOg8NZWA11p3"
      },
      "source": [
        "df2.filter(df2._c3.isNotNull()).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeL-EKam11p5"
      },
      "source": [
        "for i in df2.columns[3:]:\n",
        "  df2 = df2.withColumn(i, df2[i].cast(\"double\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCw-zU4211p6"
      },
      "source": [
        "df2.dropna().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6GLXSxK11p7"
      },
      "source": [
        "df2 = df2.filter(df2[\"_c2\"]!=\"RAINFALL\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUx3JyvT11p9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coU6Cj_S11p_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbM7XshZ2B2"
      },
      "source": [
        "# Perform Pearson Correlation using DataFrame.corr( )\n",
        "# 練習: 計算大里區pm10, pm2.5 間之關聯度 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
        "使用前面所建立的clean_weather_data rdd資料\n",
        "    \n",
        "    \n",
        "    corr(col1, col2, method=None)\n",
        "    Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
        "\n",
        "    Parameters:\t\n",
        "    col1 – The name of the first column\n",
        "    col2 – The name of the second column\n",
        "    method – The correlation method. Currently only supports “pearson”\n",
        "    New in version 1.4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5zsuLAZ2B3"
      },
      "source": [
        "def Generated_Measurement(x):\n",
        "    date = x[0]\n",
        "    location = x[1]\n",
        "    measure = x[2]\n",
        "    measurements_of_a_day = []\n",
        "    for i, value in enumerate(x[3:]):\n",
        "        measurements_of_a_day.append((date, measure, \"hr_\"+str(i), value))\n",
        "    return measurements_of_a_day\n",
        "\n",
        "daliData = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\n",
        "daliData.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8sIyUD5sBg"
      },
      "source": [
        "daliData.toDF().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49larQ_8Z2B4"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "daliDataRow = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\\\n",
        "    .flatMap(Generated_Measurement)\\\n",
        "    .map(lambda x: ( (x[0], x[2]), x[1], x[3] ) )\\\n",
        "    .groupBy(lambda x: x[0])\\\n",
        "    .filter(lambda x: len(x[1])==2)\\\n",
        "    .mapValues(lambda x: list(x))\\\n",
        "    .mapValues(lambda x: [x[0][1], x[0][2], x[1][1], x[1][2]])\\\n",
        "    .map(lambda x:[ x[0][0], x[0][1], x[1][1], x[1][3]])\\\n",
        "    .map(lambda x: Row(\n",
        "            date = x[0],\n",
        "            time = x[1],\n",
        "            pm10 = float(x[2]),\n",
        "            pm25 = float(x[3])\n",
        "        ))\n",
        "    \n",
        "df = spark.createDataFrame(daliDataRow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0vjzrg0Z2B6"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7OPjFeNZ2B7"
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCH_rb-Z2B-"
      },
      "source": [
        "df.corr(\"pm10\",\"pm25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaWYyQ_Z2CA"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FukeJJoY11Xq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}