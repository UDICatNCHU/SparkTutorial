{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "(Spark Tutorial) 上課講義 2019_Spark_DataFrame_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9FisGymJZ2BM",
        "ZQTLug-b4zwB",
        "mkGg9xJWZ2Bb",
        "xIOULjFJZ2Be",
        "CEcSbB_ZZ2BZ",
        "ycnCvcAVZ2Bi",
        "pMpQ-zrB11pp",
        "0WbM7XshZ2B2"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDICatNCHU/SparkTutorial/blob/answer/(Spark_Tutorial)_%E4%B8%8A%E8%AA%B2%E8%AC%9B%E7%BE%A9_2019_Spark_DataFrame_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFD3Nh5xZ2A-"
      },
      "source": [
        "! wget -O init_env.sh https://www.dropbox.com/s/6bnwn8u2hz19s59/init_env.sh && \\\n",
        "bash init_env.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u314rKd1Z2BC"
      },
      "source": [
        "import os, sys\n",
        "os.environ['SPARK_HOME'] = \"/usr/local/spark\"\n",
        "os.environ['PYSPARK_PYTHON'] = \"/usr/local/bin/python\"\n",
        "sys.path.append(\"/usr/local/spark/python/\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/pyspark.zip\")\n",
        "sys.path.append(\"/usr/local/spark/python/lib/py4j-0.10.4-src.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU1UwvjtaaSk"
      },
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark import SparkConf\n",
        "sc =SparkContext()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEFwbXSgauLA"
      },
      "source": [
        "!wget -O pm25.csv \"https://www.dropbox.com/s/zkn3ba7pitv83el/pm2.5Taiwan.csv?dl=0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjbkqXx0ayTw"
      },
      "source": [
        "weather = sc.textFile(\"./pm25.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9jzWplxZ2BE"
      },
      "source": [
        "weather_data_rdd = weather.map(lambda line : line.split(\",\"))\n",
        "# pm25schema = weather_data_rdd.first()\n",
        "# print(pm25schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FisGymJZ2BM"
      },
      "source": [
        "# 回想如何使用RDD計算求取2015年，大里每小時的平均pm25數值。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPMxMS1f11qI"
      },
      "source": [
        "import math\n",
        "def remove_row_with_noise (x):\n",
        "    for i in range(3, len(x)):\n",
        "        if not x[i].isdecimal():\n",
        "            return False\n",
        "    return True \n",
        "\n",
        "def hourKeyGen(x):\n",
        "    hourkeypair = []\n",
        "    x=x[3:]\n",
        "    for i, value in enumerate(x):\n",
        "      print(i, value)\n",
        "      hourkeypair.append((i, float(value)))\n",
        "    return hourkeypair\n",
        "\n",
        "clean_weather_data = weather_data_rdd\\\n",
        "                    .filter(lambda x: x!=pm25schema)\\\n",
        "                    .filter(remove_row_with_noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KGoSn_c3viJ"
      },
      "source": [
        "dalipm25 = clean_weather_data.filter(lambda x: x[2]== \"PM2.5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzMR32zDZ2BQ"
      },
      "source": [
        "count = dalipm25.count()\n",
        "HourSum = dalipm25\\\n",
        "            .flatMap(hourKeyGen)\\\n",
        "            .reduceByKey(lambda x,y: x+y)\\\n",
        "            .mapValues(lambda x: x/count)\\\n",
        "            .map(lambda x: (x[1],x[0])).top(24)\n",
        "\n",
        "print(HourSum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOldTm0PZ2BR"
      },
      "source": [
        "# 使用DataFrame 來計算每小時平均值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Peojix3wauLF"
      },
      "source": [
        "print(dalipm25.first())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLNcEsygc6oX"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3TQFM4tixp6"
      },
      "source": [
        "for i in dalipm25.take(5):\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pl9EZixkNiT"
      },
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row\n",
        "\n",
        "dalipm25row = dalipm25.map(lambda p:\n",
        "        Row(\n",
        "        date = p[0],\n",
        "        location = p[1],\n",
        "        measure = p[2],\n",
        "        hr_01 = float(p[3]), hr_02 = float(p[4]),hr_03 = float(p[5]),hr_04 = float(p[6]),hr_05 = float(p[7]),\n",
        "        hr_06 = float(p[8]), hr_07 = float(p[9]),hr_08 = float(p[10]),hr_09 = float(p[11]),hr_10 = float(p[12]),\n",
        "        hr_11 = float(p[13]),hr_12 = float(p[14]),hr_13 = float(p[15]),hr_14 = float(p[16]),hr_15 = float(p[17]),\n",
        "        hr_16 = float(p[18]),hr_17 = float(p[19]),hr_18 = float(p[20]),hr_19 = float(p[21]),hr_20 = float(p[22]),\n",
        "        hr_21 = float(p[23]),hr_22 = float(p[24]),hr_23 = float(p[25]),hr_24 = float(p[26]),\n",
        "    )\n",
        ")\n",
        "\n",
        "dalipm25row.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF182dzHZ2BU"
      },
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRw_d4rMJizR"
      },
      "source": [
        "df.filter(df[\"date\"]==\"2015/01/22\").select(\"hr_01\", \"hr_02\", \"location\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ze1VGONJlxgj"
      },
      "source": [
        "df.filter(df[\"date\"]==\"2015/01/22\").select(\"hr_01\",\"location\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H3TgwxKKAqc"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTLug-b4zwB"
      },
      "source": [
        "# 直接透過rdd生成data frame. toDF()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9la--euNJXT"
      },
      "source": [
        "pm25schema = weather_data_rdd.first()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVbTyRvDNVxl"
      },
      "source": [
        "pm25schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpycWwEed5PR"
      },
      "source": [
        "dalipm25.toDF().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpezXkfDfPNX"
      },
      "source": [
        "pm25schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gqp0-TzbAhte"
      },
      "source": [
        "df2 = dalipm25.toDF(pm25schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bku4vR-hfcT"
      },
      "source": [
        "df2.schema"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByodqKk3knMK"
      },
      "source": [
        "dfpm25 = dalipm25.toDF(pm25schema)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuzt57KyBYL5"
      },
      "source": [
        "dfpm25.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np4IHhqFyJ8Z"
      },
      "source": [
        "for i in dfpm25.columns[3:]:\n",
        "  dfpm25 = dfpm25.withColumn(i, dfpm25[i].cast(\"double\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYRCksyPll5S"
      },
      "source": [
        "dfpm25.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkGg9xJWZ2Bb"
      },
      "source": [
        "# 使用 DataFrame.filter() 來進行row資料之條件計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtr4GV5bZ2Ba"
      },
      "source": [
        "df.filter(df.hr_01>30).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIOULjFJZ2Be"
      },
      "source": [
        "# 使用 DataFrame.select() 來進行資料之Projection http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCXgch-_Z2Bc"
      },
      "source": [
        "df.select(\"hr_01\", \"hr_02\", \"location\", \"measure\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6sInWJZ2Bg"
      },
      "source": [
        "## 使用 DataFrame.describe() 來進行DataFrame or Column 資料之統計 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-vy-Ms1Z2Bh"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhbMcF9VkG66"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcSbB_ZZ2BZ"
      },
      "source": [
        "# 使用 DataFrame.agg() 來進行column數值之統計計算 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP5lLypj1P8T"
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-PTxlE0Z2Be"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "df.agg(F.mean(df.hr_01),F.mean(df.hr_02)).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofza07KUcAJ3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3jtfh2MQHTY"
      },
      "source": [
        "#使用 DataFrame.withColumn() 來進行column新增column http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaksZR0sPec"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz_KzHSQQbtB"
      },
      "source": [
        "df.withColumn('hr_avg', (df.hr_01+df.hr_02)/2).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzyEnb1kZ2CD"
      },
      "source": [
        "# 練習1: 請用SPARK SQL算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kti4CDbZ2CC"
      },
      "source": [
        "# 練習2: 請使用SPARK SQL求取2015年，全國pm2.5最高的前十個工作站測點以及其日期。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwwEAiWd7Xme"
      },
      "source": [
        "df = clean_weather_data.toDF()\n",
        "for i in df.columns[3:]:\n",
        "  df = df.withColumn(i, df[i].cast(\"double\"))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-6goC4loJ0D"
      },
      "source": [
        "avgdf = df.filter(df[\"_3\"]==\"PM2.5\").withColumn(\"dayavg\", \n",
        "                                        (\n",
        "                                        df[\"_4\"]+df[\"_5\"]+df[\"_6\"]+df[\"_7\"]+df[\"_8\"]+df[\"_9\"]+df[\"_10\"]+df[\"_11\"]\\\n",
        "                                        +df[\"_12\"]+df[\"_13\"]+df[\"_14\"]+df[\"_15\"]+df[\"_16\"]+df[\"_17\"]+df[\"_18\"]+df[\"_19\"]\\\n",
        "                                        +df[\"_20\"]+df[\"_21\"]+df[\"_22\"]+df[\"_23\"]+df[\"_24\"]+df[\"_25\"]+df[\"_26\"]+df[\"_27\"])/24)\\\n",
        "                                        .select(\"_1\", \"_2\",\"dayavg\")"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV1IM3QRoT9q"
      },
      "source": [
        "avgdf.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJZ_xlp82c5F"
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMRbvkL-2PuG"
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60).groupby(avgdf._2).count().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDq8ZLtU7Xl3"
      },
      "source": [
        "avgdf.filter(avgdf.dayavg>60)\\\n",
        "     .groupby(avgdf._2)\\\n",
        "     .count()\\\n",
        "     .orderBy('count', ascending=False)\\\n",
        "     .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUg4ZNDN3n4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycnCvcAVZ2Bi"
      },
      "source": [
        "# 使用 Spark SQL來下達SQL查詢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfO7cTEUZ2Bj"
      },
      "source": [
        "df = spark.createDataFrame(dalipm25row)\n",
        "df.show()\n",
        "df.registerTempTable(\"Table\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSsE-3AWZ2Bl"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "            select * from Table where date ='2015/02/07'\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66vVEpvUZ2Bo"
      },
      "source": [
        "s = spark.sql(\"\"\"\n",
        "                select count(*) count \n",
        "                from Table \n",
        "                where hr_01 > 100\n",
        "               \"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxauml1-Z2Bt"
      },
      "source": [
        "type(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDPUeJQp6Ej_"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date, location \n",
        "                from Table\n",
        "                where date=\"2015/02/07\"\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jux8542WZ2Bv"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select date,location, hr_01, hr_02, hr_01-hr_02 as diff \n",
        "                from Table\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQg8tWY8ty-B"
      },
      "source": [
        "## 練習1: 請使用SparkSQL，算算看2015全國哪個測站，紫爆天數最多？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6yIB8oitx1f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgoKjn6PZ2Bw"
      },
      "source": [
        "expr=\"\"\n",
        "for i in df.columns[1:25]:\n",
        "  expr=str(i)+\"+\"+expr\n",
        "\n",
        "print(expr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzr5S_ysZ2By"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "                select location, date, (hr_24+hr_23+hr_22+hr_21+hr_20+hr_19+hr_18+hr_17+hr_16+hr_15+hr_14+hr_13+hr_12+hr_11+hr_10+hr_09+hr_08+hr_07+hr_06+hr_05+hr_04+hr_03+hr_02+hr_01)/24 as avg\n",
        "                from Table \n",
        "                having avg>60\n",
        "          \"\"\").registerTempTable(\"DayAvgTable\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVcomNQfZ2B0"
      },
      "source": [
        "spark.sql(\"\"\"\n",
        "          select location, count(date) as count\n",
        "          from DayAvgTable\n",
        "          group by location\n",
        "          order by count DESC\n",
        "          \"\"\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HryYFAWXZ2B1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMpQ-zrB11pp"
      },
      "source": [
        "# 利用DataFrame 直接讀取CSV, Fillna, DropNa功能清洗資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO_EEY1011pq"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql import Row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pDYpUS811ps"
      },
      "source": [
        "df = spark.read.load(\"./pm25.csv\", format=\"csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBn5I6iY11pt"
      },
      "source": [
        "df.show(200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Du-yrk11pw"
      },
      "source": [
        "df2=df.filter(df[\"_c0\"]!=\"日期\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eRUGtKK11py"
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcL55Mg211p0"
      },
      "source": [
        "df2.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CN4CwP_11p2"
      },
      "source": [
        "df2.count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOg8NZWA11p3"
      },
      "source": [
        "df2.filter(df2._c3.isNotNull()).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeL-EKam11p5"
      },
      "source": [
        "for i in df2.columns[3:]:\n",
        "  df2 = df2.withColumn(i, df2[i].cast(\"double\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCw-zU4211p6"
      },
      "source": [
        "df2.dropna().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6GLXSxK11p7"
      },
      "source": [
        "df2 = df2.filter(df2[\"_c2\"]!=\"RAINFALL\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUx3JyvT11p9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coU6Cj_S11p_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WbM7XshZ2B2"
      },
      "source": [
        "# Perform Pearson Correlation using DataFrame.corr( )\n",
        "# 練習: 計算大里區pm10, pm2.5 間之關聯度 http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
        "使用前面所建立的clean_weather_data rdd資料\n",
        "    \n",
        "    \n",
        "    corr(col1, col2, method=None)\n",
        "    Calculates the correlation of two columns of a DataFrame as a double value. Currently only supports the Pearson Correlation Coefficient. DataFrame.corr() and DataFrameStatFunctions.corr() are aliases of each other.\n",
        "\n",
        "    Parameters:\t\n",
        "    col1 – The name of the first column\n",
        "    col2 – The name of the second column\n",
        "    method – The correlation method. Currently only supports “pearson”\n",
        "    New in version 1.4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW5zsuLAZ2B3"
      },
      "source": [
        "def Generated_Measurement(x):\n",
        "    date = x[0]\n",
        "    location = x[1]\n",
        "    measure = x[2]\n",
        "    measurements_of_a_day = []\n",
        "    for i, value in enumerate(x[3:]):\n",
        "        measurements_of_a_day.append((date, measure, \"hr_\"+str(i), value))\n",
        "    return measurements_of_a_day\n",
        "\n",
        "daliData = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\n",
        "daliData.cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y8sIyUD5sBg"
      },
      "source": [
        "daliData.toDF().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49larQ_8Z2B4"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "daliDataRow = clean_weather_data.filter(lambda x: x[1]==\"大里\" and (x[2] == \"PM2.5\" or x[2] == \"PM10\" ))\\\n",
        "    .flatMap(Generated_Measurement)\\\n",
        "    .map(lambda x: ( (x[0], x[2]), x[1], x[3] ) )\\\n",
        "    .groupBy(lambda x: x[0])\\\n",
        "    .filter(lambda x: len(x[1])==2)\\\n",
        "    .mapValues(lambda x: list(x))\\\n",
        "    .mapValues(lambda x: [x[0][1], x[0][2], x[1][1], x[1][2]])\\\n",
        "    .map(lambda x:[ x[0][0], x[0][1], x[1][1], x[1][3]])\\\n",
        "    .map(lambda x: Row(\n",
        "            date = x[0],\n",
        "            time = x[1],\n",
        "            pm10 = float(x[2]),\n",
        "            pm25 = float(x[3])\n",
        "        ))\n",
        "    \n",
        "df = spark.createDataFrame(daliDataRow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0vjzrg0Z2B6"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7OPjFeNZ2B7"
      },
      "source": [
        "df.groupBy().avg().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOCH_rb-Z2B-"
      },
      "source": [
        "df.corr(\"pm10\",\"pm25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVaWYyQ_Z2CA"
      },
      "source": [
        "df.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FukeJJoY11Xq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}